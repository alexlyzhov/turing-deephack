{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format\n",
    "===\n",
    "\n",
    "The data itself is already pre-processed (`<s>, </s>` tags, `<unk>` tag, etc.). The punctuation is tokenized (one symbol = one token, no word-punctuation merged tokens). The is only one space symbol between every two adjacent tokens.\n",
    "\n",
    "UPD: In the data there are common combination \"@@ \". This is because the data has been preprocessed with BPE encoding (see: https://arxiv.org/abs/1508.07909 and https://github.com/rsennrich/subword-nmt) in order to reduce the vocabulary size.\n",
    "\n",
    "In order to properly print a message you should make sure to do the following in Python:\n",
    "\n",
    "[your string message here].replace(‘@@ ‘, ‘’)\n",
    "\n",
    "NOTE: replace `@@space` by nothing. Don’t forget the [space]!\n",
    "\n",
    "Broadly speaking, BPE encoding will split words into the most common n-gram to reduce the vocabulary size. The ‘@@ ’ you see are tokens to indicate there was a split. Thus to print the actual word you should replace all occurrences of '@@' to nothing.\n",
    "\n",
    "\n",
    "Data fields are separated by one tab character.\n",
    "\n",
    "    context - context phrase(s) for response, always human generated\n",
    "    response - one phrase or a few phrases, may be from different speakers\n",
    "    human-generated - flag if the response is generated by human\n",
    "\n",
    "Data header: 'id\\tcontext\\tresponse\\thuman-generated\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission format\n",
    "===\n",
    "\n",
    "ROC AUC score.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "id,human-generated\n",
    "\n",
    "1,1\n",
    "\n",
    "8,0\n",
    "\n",
    "9,1\n",
    "\n",
    "10,1\n",
    "\n",
    "We expect the solution file to have 524,342 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(frame):\n",
    "    frame_features = []\n",
    "    \n",
    "    for index, row in frame.iterrows():\n",
    "        context = row['context']\n",
    "        response = row['response']\n",
    "        features = []\n",
    "    \n",
    "        features.append(float(len(response)))\n",
    "\n",
    "        tokens = response.split(' ')\n",
    "        features.append(float(len(tokens)))\n",
    "        \n",
    "        frame_features.append(features)\n",
    "\n",
    "    return frame_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 20000\n",
    "all_features = []\n",
    "all_labels = []\n",
    "for i, frame in enumerate(pd.read_csv('data/train.txt', chunksize = chunksize, delimiter = '\\t')):\n",
    "    frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    labels = frame['human-generated'].values.tolist()\n",
    "    \n",
    "    all_features.extend(features) # actually more efficient than numpy append\n",
    "    all_labels.extend(labels)\n",
    "    \n",
    "    if i > 2: break # to lessen number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_features = np.array(all_features)\n",
    "np_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(np_features, np_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 20000\n",
    "all_features = []\n",
    "all_ids = []\n",
    "for i, frame in enumerate(pd.read_csv('data/test.txt', chunksize = chunksize, delimiter = '\\t')):\n",
    "    frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    ids = frame['id'].values.tolist()\n",
    "    \n",
    "    all_features.extend(features) # actually more efficient than numpy append\n",
    "    all_ids.extend(ids)\n",
    "    \n",
    "    if i > 2: break # to lessen number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_features = np.array(all_features)\n",
    "np_ids = np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(np_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.concatenate(([np_ids], [predicted_labels]), axis = 0).T, columns = ['id', 'human-generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

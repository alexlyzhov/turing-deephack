{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format\n",
    "===\n",
    "\n",
    "The data itself is already pre-processed (`<s>, </s>` tags, `<unk>` tag, etc.). The punctuation is tokenized (one symbol = one token, no word-punctuation merged tokens). The is only one space symbol between every two adjacent tokens.\n",
    "\n",
    "UPD: In the data there are common combination \"@@ \". This is because the data has been preprocessed with BPE encoding (see: https://arxiv.org/abs/1508.07909 and https://github.com/rsennrich/subword-nmt) in order to reduce the vocabulary size.\n",
    "\n",
    "In order to properly print a message you should make sure to do the following in Python:\n",
    "\n",
    "[your string message here].replace(‘@@ ‘, ‘’)\n",
    "\n",
    "NOTE: replace `@@space` by nothing. Don’t forget the [space]!\n",
    "\n",
    "Broadly speaking, BPE encoding will split words into the most common n-gram to reduce the vocabulary size. The ‘@@ ’ you see are tokens to indicate there was a split. Thus to print the actual word you should replace all occurrences of '@@' to nothing.\n",
    "\n",
    "\n",
    "Data fields are separated by one tab character.\n",
    "\n",
    "    context - context phrase(s) for response, always human generated\n",
    "    response - one phrase or a few phrases, may be from different speakers\n",
    "    human-generated - flag if the response is generated by human\n",
    "\n",
    "Data header: 'id\\tcontext\\tresponse\\thuman-generated\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission format\n",
    "===\n",
    "\n",
    "ROC AUC score.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "id,human-generated\n",
    "\n",
    "1,1\n",
    "\n",
    "8,0\n",
    "\n",
    "9,1\n",
    "\n",
    "10,1\n",
    "\n",
    "We expect the solution file to have 524,342 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # download all nltk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 50000\n",
    "train_filename = 'data/sampled_train.txt' # shrunk by sample_train.sh to 10% of the size, random rows\n",
    "full_train_filename = 'data/train.txt'\n",
    "test_filename = 'data/test.txt'\n",
    "eval_filename = 'data/eval.txt' # shrunk by sample_eval.sh to 50000 random rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(function, filename, print_every = 5, chunks = None):\n",
    "    # run function on every chunk of file\n",
    "    start = time()\n",
    "    for i, frame in enumerate(pd.read_csv(filename, chunksize = chunksize, delimiter = '\\t')):\n",
    "        frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "        \n",
    "        function(frame)\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print('Chunk ' + str(i) + ' over: ' + str(round(time() - start)) + 's', end = '\\t')\n",
    "        if chunks is not None:\n",
    "            if i >= chunks - 1:\n",
    "                break\n",
    "    print('\\n' + str(i + 1) + ' chunks overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "possible_tags = [',', '$', \"''\", '(', ')', ',', '--', '.', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR',\n",
    "                'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS',\n",
    "                'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
    "# syntactical tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(frame): # for a single chunk of data\n",
    "    frame_features = []\n",
    "    \n",
    "#     for index, row in frame.iterrows():\n",
    "#     for row in frame.itertuples(): # requires 'dot access' to columns\n",
    "#     zip is faster than itertuples and a whole lot faster than iterrows\n",
    "    for context, response in zip(frame['context'], frame['response']):\n",
    "        features = []\n",
    "        \n",
    "        response_tokens = response.split(' ')\n",
    "        context_tokens = context.split(' ')\n",
    "    \n",
    "        # feature: response length\n",
    "        features.append(float(len(response)))\n",
    "\n",
    "        # feature: number of tokens in the response\n",
    "        features.append(float(len(response_tokens)))\n",
    "    \n",
    "        # feature: context length\n",
    "        features.append(float(len(context)))\n",
    "\n",
    "        # feature: number of tokens in the context\n",
    "        features.append(float(len(context_tokens)))\n",
    "        \n",
    "        # features: number of words of length 1, 2, ..., 5 in response\n",
    "        # counters of length 6+ don't seem to have importance\n",
    "        lens = Counter(map(len, response_tokens))\n",
    "        for i in range(5):\n",
    "            features.append(float(lens[i + 1]))\n",
    "        \n",
    "        # features: number of words of length 1, 2, ..., 5 in context\n",
    "        lens = Counter(map(len, context_tokens))\n",
    "        for i in range(5):\n",
    "            features.append(float(lens[i + 1]))\n",
    "        \n",
    "        # features: number of specific tokens from the list in the response\n",
    "        tokens_to_count = ['<at>', '<number>', '!', '.']\n",
    "        for token_to_count in tokens_to_count:\n",
    "            features.append(float(response_tokens.count(token_to_count)))\n",
    "            \n",
    "        # feature: number of tweets in context\n",
    "#         new_tweet_marks = ['<first_speaker>', '<second_speaker>', '<third_speaker>', '<minor_speaker>']\n",
    "# first and third are unimportant so:\n",
    "        new_tweet_marks = ['<second_speaker>', '<minor_speaker>']\n",
    "        context_tweet_count = 0\n",
    "        for mark in new_tweet_marks:\n",
    "            context_tweet_count += context_tokens.count(mark)\n",
    "        features.append(float(context_tweet_count))\n",
    "            \n",
    "        # features: includes apostrophes, is composed of english letters, is in vocabulary, is a stopword\n",
    "        # for context and response\n",
    "        counters = [0] * 8\n",
    "        for token in response_tokens:\n",
    "            if \"'\" in token:\n",
    "                counters[0] += 1\n",
    "            if token.isalpha():\n",
    "                counters[1] += 1\n",
    "            if token in vocab:\n",
    "                counters[2] += 1\n",
    "            if token in stopwords:\n",
    "                counters[3] += 1\n",
    "        for token in context_tokens:\n",
    "            if \"'\" in token:\n",
    "                counters[4] += 1\n",
    "            if token.isalpha():\n",
    "                counters[5] += 1\n",
    "            if token in vocab:\n",
    "                counters[6] += 1\n",
    "            if token in stopwords:\n",
    "                counters[7] += 1\n",
    "        for counter in counters:\n",
    "            features.append(float(counter))\n",
    "            \n",
    "        all_tokens = set(context_tokens + response_tokens)\n",
    "        shared_tokens = set(context_tokens).intersection(response_tokens)\n",
    "\n",
    "        # feature: number of shared tokens between context and response\n",
    "        features.append(float(len(shared_tokens)))\n",
    "\n",
    "        # feature: sum of abs diffs in token shares\n",
    "        diff = 0\n",
    "        for token in all_tokens:\n",
    "            token_stat = abs(float(context_tokens.count(token)) / len(context_tokens) -\n",
    "                         float(response_tokens.count(token)) / len(response_tokens))\n",
    "            diff += token_stat\n",
    "        features.append(diff)\n",
    "        \n",
    "#         # features: counts of syntactical tags (INEFFICIENT)\n",
    "#         word_tokens = list(filter(lambda token: '<' not in token, response_tokens))\n",
    "# #         print(word_tokens)\n",
    "#         if len(word_tokens) > 0:\n",
    "#             tags = list(zip(*nltk.pos_tag(word_tokens)))[1]\n",
    "#         else:\n",
    "#             tags = []\n",
    "#         tag_features = [float(tags.count(tag)) for tag in possible_tags]\n",
    "#         features.extend(tag_features)\n",
    "        \n",
    "        frame_features.append(features)\n",
    "\n",
    "    return frame_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_frame(print_filename): # get first chunk in a file as a dataframe for exploration\n",
    "    for frame in pd.read_csv(print_filename, chunksize = chunksize, delimiter = '\\t'):\n",
    "        frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "        break\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = first_frame(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# short_frame = frame[['context', 'response']]\n",
    "# f = extract_features(short_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human\n",
      "<first_speaker> just bottled my first beer from a recipe by me . let 's see how it improves during the next weeks . but it tastes f * * ng amazing just now ! ! ! ! ! <second_speaker> <at> congrats ! and cheers ! and where 's the photo ?\n",
      "\n",
      "<first_speaker> <at> i 'll upload it tomorrow . my iphone is out of battery\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i don 't even understand how people can dislike katie ?\n",
      "\n",
      "<second_speaker> <at> blame xfactor they edited all videos to make katie look spoilt and favoured . as we all no she aint like that at all\n",
      "=====\n",
      "Human\n",
      "<first_speaker> voice thread users . if i create multiple identities , give people access to my account , they can edit collaborate simultaneously right ? <second_speaker> <at> yes they can edit , simultaneously ? not sure yet . my students have their own free accounts and can edit a thread .\n",
      "\n",
      "<first_speaker> <at> but they can 't co - create . i 'm doing this with a group of teachers this week and want them to build one together .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> <at> <at> <at> <at> <at> <at> <at> <at> f4f ?\n",
      "\n",
      "<second_speaker> <at> follow you follow you\n",
      "=====\n",
      "Human\n",
      "<first_speaker> if you don 't have server access , can you use css to control the mobile view redirect ? sphelp sharepoint <second_speaker> <at> css would be styles only . what are you trying to do ?\n",
      "\n",
      "<first_speaker> <at> trying to keep mobile users from redirecting to the mobile page . i don 't have server access , so the magic will happen client side\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i 'm wondering why i don 't have a new tattoo . where 's <at> ? lololol . <second_speaker> <at> lol , i didn 't get the number from the chic yet . she was there .\n",
      "\n",
      "<first_speaker> <at> it 's on his blog . lol .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> no to conseguindo te aceitar no facebook , o negocio piro pasodkaspok <second_speaker> <at> vai entender s as vezes acontece isso comigo tambm auisaiuhsauhisuaihsiausa\n",
      "\n",
      "<first_speaker> <at> i love you\n",
      "=====\n",
      "Human\n",
      "<first_speaker> what 's good in the cinema atm ? except for justin bieber , already seen that <second_speaker> <at> paul is good <first_speaker> <at> i no but its a <number> and i 'm going with my <number> year old brother <second_speaker> <at> darn cant u hide him or sneak him in ? lol what about gnomeo and juliet ? or yogi bear haha <first_speaker> <at> i think were gonna go see big momma <second_speaker> <at> oh yeh how could i forget that lemme know if its any good <first_speaker> <at> its amazing , so funny . i loved it\n",
      "\n",
      "<second_speaker> <at> great im going to go see it then\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> i just asked my brother for a bj so he pushed me off the sofa . i don 't think he realises a bj means a back job aswell . <second_speaker> <at> i love cheese . <first_speaker> <at> that 's made me laugh so much <second_speaker> <at> lol . that tweet was a little tooo freaky for me ! <first_speaker> <at> hahaa , i 'm sorry ! i 'll learn my lesson\n",
      "\n",
      "<second_speaker> <at> well . i bet it wasn 't too bad .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> go to bed <heart> rt <at> dead . <second_speaker> <at> im in bed ! <first_speaker> <at> well smart ass go to sleep ! <second_speaker> <at> why you aint <first_speaker> <at> its hard for me to go to sleep early because of my shift <second_speaker> <at> blah blah blah <first_speaker> <at> shuddup & take yo sucka typ self to sleep <second_speaker> <at> how rude\n",
      "\n",
      "<second_speaker> <at> i believe it , once you 've been introduced to me it 's hard to find a replacement . lol juuuuussst kiddiing we miss u too homie\n",
      "=====\n",
      "Human\n",
      "<first_speaker> tweet me if you want me to give the wanted by hand in front of them face to face your twitter usernames meetin them on <number> th june d retweet <second_speaker> <at> could you give them my twitter name ? o d xx <first_speaker> <at> of course hun d x <second_speaker> <at> aw thankyou soo much ! ' d xx\n",
      "\n",
      "<first_speaker> <at> no problenm hun its all about the twfanmily d x\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> celine imy ! d <second_speaker> <at> i missss you tooo i haven 't seen you since forever ! ! <first_speaker> <at> iknow ! we need to chill this summer & me & valerie visited you today but i think you wernt home <second_speaker> <at> she told me and i wasn 't p <first_speaker> <at> oh okay <second_speaker> <at> we need to hangout at your house and do ninja shit\n",
      "\n",
      "<second_speaker> <at> which makes it hotter\n",
      "=====\n",
      "Human\n",
      "<first_speaker> party tonight <second_speaker> <at> where <first_speaker> <at> ima text you\n",
      "\n",
      "<second_speaker> <at> lol at the message i sent you\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> i don 't hate you <second_speaker> <at> those tweets made my whole family laugh\n",
      "\n",
      "<second_speaker> <at> why do you hate your family ? ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> if chris ashton being the new jason robinson means he 's going to score a try in the <number> world cup final then i 'd be happy with that ! rugby <second_speaker> <at> the english score tries at world cups ? <first_speaker> <at> one was enough in <number>\n",
      "\n",
      "<second_speaker> <at> preceding the biggest drop in good playing style rugby has ever seen .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> ff blessings to talented artists <at> <at> <at> <at> <at> <at> <at>\n",
      "\n",
      "<second_speaker> <at> thanks for the ff !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> maggie got her shoes back on <url>\n",
      "\n",
      "<second_speaker> <at> so what you doing tonight ? let 's go home !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> hello fellow god parent to florence <second_speaker> <at> and hello fellow audi owner ! pick my new a1 up next week ! ! ! love it\n",
      "\n",
      "<first_speaker> <at> enjoy the new car smell and completely unnecessary journeys ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> ummm the stores don 't carry the balenciaga spring set ? they didn 't know what i was talking about .\n",
      "\n",
      "<second_speaker> <at> it sounded so nice ! i love that the uke can sound like that\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i don 't give a fuck point blank . ill be on that other shit . <second_speaker> <at> whats goimg on sis i neef to make a trip to akron ? <first_speaker> <at> no its cool people just acting really funny\n",
      "\n",
      "<second_speaker> <at> ok dont let them get to you . fuck emm and keep it moving . love ya sis\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> whst you up to . ?\n",
      "\n",
      "<second_speaker> <at> watchin da game you ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> i have smartly managed to sidetrack a boring meeting . i feel like smooching myself . <second_speaker> <at> i can 't stop laughing . my mom thinks i 'm dumb .\n",
      "\n",
      "<second_speaker> <at> i seriously hopeit 's just a rumor ? ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> we believe in better bahrian , nighty <second_speaker> <at> just dont give up hope or lulu until you get what you want . lulu is what keeps the west 's attention on bahrain .\n",
      "\n",
      "<first_speaker> <at> i sometimes say that with stupidity and i need to know what happens .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> when i was a freshie , my fas was a handsome malay and a ahbeng chinese . <second_speaker> <at> when i was a freshie , my fas was a handsome malay and a ahbeng chinese . \" what happened to the malay fa . <at> <first_speaker> <at> he became ugly . - - & gt <at>\n",
      "\n",
      "<third_speaker> <at> <at> o . o\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> fuckin up the carpet spilling his drink nshit\n",
      "\n",
      "<second_speaker> <at> lol i know right ! ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> working out in thie pool everyday till i get back . then keep doing my usual miles in the morning dedicated\n",
      "\n",
      "<second_speaker> <at> yeah you need to tell your cousin to work out lazyboysyndrome\n",
      "=====\n",
      "Human\n",
      "<first_speaker> yyyyyyyy <at> <second_speaker> <at> huh ? <first_speaker> <at> i heard what happen <second_speaker> <at> w me and your cuz ?\n",
      "\n",
      "<first_speaker> <at> nope , but what happened with that\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> my passport expired on my birthday . the nation of canada won 't let me in . i am a danger . any ideas ? government offices closed on sunday .\n",
      "\n",
      "<second_speaker> <at> . talk about ice hockey and maple syrup , then hope for the best ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> miss tweeting with <at> ! !\n",
      "\n",
      "<second_speaker> <at> hi girl , a little birdie told me you missed me\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> what 's the recommended way to deploy so that you don 't have to run buildout on production server ? plone python\n",
      "\n",
      "<second_speaker> <at> hey ! how are you ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> truth rt <at> furthermore , rappers . if you only listen to rap , you suck at rapping . i promise .\n",
      "\n",
      "<second_speaker> <at> is it is that somebody he wants to take care with is the newer album ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> back to blackops , it been like <number> weeks lol <second_speaker> <at> boring game\n",
      "\n",
      "<first_speaker> <at> it 's been a long time since i 've been there lol\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> like your new pic ! - p <second_speaker> <at> ha ! thanks ! tho everytime i look at it i 'm like why did i let my face get that close to that nasty , weird smelling water ? ! ? ! <first_speaker> <at> i know . is that a drinking fountain ? i couldn 't tell <second_speaker> <at> i think it 's just decorative . maybe originally for drinking though ? idk , but it 's on the u of o campus and its from the ' 20s ! <first_speaker> <at> wow that 's pretty cool how 's your summer going ? <second_speaker> <at> oh good . waitress quit w out notice now there 's only <number> of us for the rest of summer . im counting days to fall ! ! hows yours ? <first_speaker> <at> its good . do u work for a national chain or a local eatery ? how did u do on your finals ? <second_speaker> <at> i got an a & a b on my finals ! i work in an awesome little mom & pop type fish n ' chips joint and i love it there ! <first_speaker> <at> congratulations ! sure wish stevie would do a west coast date so we could celebrate ! - p <second_speaker> <at> hahaha ! ! right ? ! ? if only i had the time & money for the colorado show ! ! <first_speaker> <at> u & me both ! i 'm gonna check out tix on stubhub . i really wanna see her\n",
      "\n",
      "<second_speaker> <at> did you find anything ? i 'm having a serious crisis of do i wanna pay for another term of college or go see stevie in co ? lol .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> hmmm it appears as though i 'll be staying in tonight . refuse to spend another saturday night going out with parents for dinner . <second_speaker> <at> a cutie like you doesn 't have someone ? <first_speaker> <at> no sir . perpetually single - <second_speaker> <at> . intelligent , successful , driven , personable , and attractive women have to say that . i can 't call it . <first_speaker> <at> idk what the deal is ! i 'm not single by choice tho ! i took about <number> mos of \" me \" time to battle health issues so i 'm good to go ! <second_speaker> <at> well we haven 't been chattin long but i have good senses and can assess ppl pretty quickly and can 't see it lasting long <first_speaker> <at> i hope you 're right ! but i dunno . i know exactly what i 'm looking for so i guess i 'm kinda picky <second_speaker> <at> when you 're the one being chased and have options therein , you have the right to be choosey . what may i ask are your criteria ? <first_speaker> <at> after those <number> , very laid back , tall - ish , athletic & into sports but not obsessed , good sense of humor , motivated hard worker <second_speaker> <at> well i miss out on the tall - ish one , but i fit the rest . lol all of that is attainable , though . <first_speaker> <at> i 'm <number> ' 8 so its relatively importantish . <number> ' 11 + is ideal but it 's negotiable ! <second_speaker> <at> understood and acceptable . i gotcha . - <first_speaker> <at> list isn 't toooo unreasonable i don 't think ! ! <second_speaker> <at> no , not at all . i need to ask one more thing , though . not every weekend means you don 't want it often ? <first_speaker> <at> haha it just means i don 't want <number> am drunk phone calls every weekend saying slurring come over i miss you , etc . lol <second_speaker> <at> ahh . that 's different . well as long as \" it \" is regular , none of that is out unreasonable or out of the question . <first_speaker> <at> yeaaahhh the <number> am stuff has never sat well with me . especially when he knows i 'm not out & asleep ! but otherwise , i 'm in ! haha <second_speaker> <at> like i said , as long as there 's adequate alone time , a good man will find you and fill those criteria . <first_speaker> <at> fingers crossed ! ! i don 't mind heading out on the weekends but generally speaking , theyre chill time for me so plenty of time ! <second_speaker> <at> i 'm not big on goin out all the time either , so i know i 'd fit that criteria as well . lol <first_speaker> <at> you fit most of my ciritia ! !\n",
      "\n",
      "<second_speaker> <at> i know , i know . i 'm just saying . i don 't think i 'm going to be able to do that . i don 't know what i 'm doing .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> been following the canucks lately . i 'm starting to like them ? my bruins are still my number oneeeee thoughhh <second_speaker> <at> they winning tonight ? ? baskeball\n",
      "\n",
      "<first_speaker> <at> celts swept the knicks\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> <at> i love dancing of any kind ! i hear music , and can 't not move ! ! ! ! fitstudio <second_speaker> <at> i love to move , awkwardly , but i move ! fitstudio <third_speaker> <at> you cannot be as uncoordinated as i am . fitstudio\n",
      "\n",
      "<second_speaker> <at> i 'm pretty klutzy . like today , hand leg go as i was moving coffee grounds to the trash . <number> ft away . coffee everywhere fitstudio\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> new favourite i phone app - hey tell love it . having fun chatting w my kid <second_speaker> <at> i just downloaded it , will try it out <first_speaker> <at> u still have my cell on fb msg ? u can talk to me ! <second_speaker> <at> omg the kids loved that ! they kept answering you while i was playing the message chat to you later . <first_speaker> <at> fun fun ! so cute talking to them ! ! very articulate young boys love it ! ! fun new ap yes ! <second_speaker> <at> thank you , so fun , am just about to send link to inlaws so kids can chat to nan & pop . thanks for the recommendation <first_speaker> <at> no prob ! i mean skype is easy via phone too but this is fun ! <second_speaker> <at> i don 't always look pretty enough for skype facetime , i prefer this\n",
      "\n",
      "<first_speaker> <at> yeah i know ! ! ! ! ! ! ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> hey sweetie i finally found someone who likes the jonas brothers d follow me back and we can tweet each other <second_speaker> <at> followed <first_speaker> <at> thanke u how are u which is the favourite of the jonas brothers for u ?\n",
      "\n",
      "<second_speaker> <at> i 'm good thanks , how are you ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> imagine you have been dating james for a year now . it 's your anniversary . james takes you out for a specia <heart> <cont> <url>\n",
      "\n",
      "<second_speaker> <at> lmaoo , i shhhhhh had a vagina ! ha . thanks . i just think thats the one they left it up !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> i love you so much , shane . my life would suck without you . and you can 't imagine how happy i would be to get a reply . <second_speaker> <at> thanks ! ! your awesome ! !\n",
      "\n",
      "<third_speaker> i love <at> he 's like so funny , and cute . i wonder if hed ever reply to me ? probably not\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> e . rutherford <at> <number> <number> <at> pre - sale now <url>\n",
      "\n",
      "<second_speaker> <at> i was in at <number> <number> pm and out with my <number> * at <number> <number> pm very smooth thank u !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i feel like im gonna be up all night smh <second_speaker> <at> yu out here <first_speaker> <at> yepppp <second_speaker> <at> come say happy bday to nicole\n",
      "\n",
      "<first_speaker> <at> is it raining ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> we have our <number> winners . will reveal winners tmrw and i will follow you so that you can dm me your address for boots and shirts .\n",
      "\n",
      "<second_speaker> <at> can you follow me back please ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> . <at> , are vinny 's manly parts worth it ? jerseyshore <at> <url> <second_speaker> <at> ok u need jesus . <first_speaker> <at> that ain 't nothing my mama never told me at <number> am standing over my bed with holy water .\n",
      "\n",
      "<second_speaker> <at> * lol * u are silly . when is that part <number> of the nene interview coming ? she just let it flow !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> ew . he just called me by his gf 's name & gt . & lt\n",
      "\n",
      "<second_speaker> <at> who is he ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> disney world in two weeks . excited to get drunk around the world and spend some qt with <at> . <second_speaker> <at> . is that what you call it ? <heart>\n",
      "\n",
      "<first_speaker> <at> damn right that 's what i call it . quality time with you , mickey and minnie\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> i am doing the imperial application ambitious <second_speaker> <at> bit early for wib , isn 't it ?\n",
      "\n",
      "<third_speaker> <at> <at> <at> it 's a fucking joke haha .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> there is no doubt in my mind that buddy is <number> % for sure making my wedding cake . cakeboss & lt <number> <second_speaker> <at> i live about <number> mins from his bakery . i totally want to visit ! lol <first_speaker> <at> really ! ? that is so cool ! you 'll have to visit for both of us . if you get a cake you so have to show me it . <second_speaker> <at> i definitely will ! maybe a mini cake b c his cakes are pretty expensive ! lol <first_speaker> <at> i was wondering how much they were ? i bet they are . <second_speaker> <at> it depends on how big and elegant but if your talking about a birthday cake or something probably at least $ 1 , <number> <first_speaker> <at> gosh ! o . o but he does gret work so . haha ! i was just kidding with you . there a crazy bunch . <second_speaker> <at> lol yea the cakes are a lot of $ $ but i am sure worth it ! and the cast is crazy . crazy annoying ! lol <first_speaker> <at> my name is patricia aka tricia if you need a name to put on it . lmao . jk ! to much drama ! so anoying . <second_speaker> <at> haha u can expect a cake at ur door step hehe and yes , they are wayyyy too much drama ! <first_speaker> <at> omg ! we should start our own cake shop ! lol i hope you can bake ' cause i can 't . i can see us covered in cake mix . lol <second_speaker> <at> lol totally ! it would def . be an adventure and tv worthy lol . esp since neither of us claim to be amazing bakers ! haha <first_speaker> <at> whoo ! number one reality tv show stars , here we come ! * fist pump * & lt lol isn 't josh a good cook baker ? lol <second_speaker> <at> haha * fist pump bakery * haha ! & yea he is good , he could be there to make sure we don 't burn the place down <first_speaker> <at> yes we def . need make sure he 's on board ! lol we also need more people to help . maybe we can hire buddy ? lol <second_speaker> <at> haha yea he can come on by and whoever else is semi - good at baking ! lol <first_speaker> <at> we can get the girls that do the cupcakes ? lol i forgot what their show was called . <second_speaker> <at> lol we can do that , easy enough , i hope ! <first_speaker> <at> we 're saved ! buddy 's bakery teaches classes ! lol i went on their website to see how much a cake would cost and saw that ! lol <second_speaker> <at> haha there you go ! we 're all set ! <first_speaker> <at> - closed ! lol . <second_speaker> <at> haha they might cry a little . idk , maybe were awesome and we haven 't pushed ourselves enough .\n",
      "\n",
      "<first_speaker> <at> lol ! lol . well . you keep going out of the way to florida to drive .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> now buzzing the clippings mother who drowned family tried to back out of hudson river abc news <url>\n",
      "\n",
      "<second_speaker> <at> very heartbreaking . why would she do that , prayin for her and her family\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> rawwwwrrr scaryy huh ? <second_speaker> very lmaoo rt <at> rawwwwrrr scaryy huh ? <first_speaker> <at> muhahahaha & gt yesss ! <second_speaker> <at> lmaoo . i was kidding . mwahaha . <first_speaker> <at> - _ _ _ _ _ - ahh u fxcking liar lol <second_speaker> <at> lol sowwyyyy <first_speaker> <at> eat some cake ! lol <second_speaker> <at> ewww . i hate that shit . you know that lmao . <first_speaker> <at> lmaoo iknoww <second_speaker> <at> lol you 're evil . <first_speaker> <at> lol y thank u ^ _ ^ <second_speaker> <at> lol silly . <first_speaker> <at> meowwww lol now that 's some scary shxt sound like a tiger <second_speaker> <at> lmaoo tigers don 't meow . <first_speaker> <at> what they do then since u wanna be a smart al - lick <second_speaker> <at> lol they don 't though . they rawrrrrrr . <first_speaker> <at> nooo that 's gold fishes <second_speaker> <at> lmaooo oh yeah . don 't pigs ribbit ? <first_speaker> <at> noo that 's a chair <second_speaker> <at> lmfao okay , i 'm lost now dashawn . <first_speaker> <at> girll tiger meow chair do watever u said gold fish rawrr it 's just that simple dawn <second_speaker> <at> lol oh okay , i gotcha now ! <first_speaker> <at> lol yup ^ _ ^ and i like zip lock bags <second_speaker> <at> mee toooo . te best bags ever . <first_speaker> <at> iknowww better then the brown bags\n",
      "\n",
      "<second_speaker> <at> lmao one $ $ paint woogooo boy\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> ff <at> <at> <at> your tweets make me smile\n",
      "\n",
      "<second_speaker> <at> can you make me smile , by following me please ? ? x\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> oh shit , i passed <at> up in tweets ! ftmfw <second_speaker> <at> i still got more followers than you <first_speaker> <at> i ain 't say shit about followers . - _ _ _ - <second_speaker> <at> so ? that 's more important than tweets lol <first_speaker> <at> fuck up . <second_speaker> <at> imjustsaying what 's the point of having a bunch of tweets and no followers to read them ? oh okay <first_speaker> <at> well , at least my about of followers and the amount fo people i 'm following are around the same . <second_speaker> <at> mine are too dummy smh <first_speaker> <at> point exactly . <second_speaker> <at> stfu lol you make no sense at all <first_speaker> <at> fuck up <second_speaker> <at> bitch , you mad ?\n",
      "\n",
      "<first_speaker> <at> no i 'm not\n",
      "=====\n",
      "Human\n",
      "<first_speaker> nytie mrng tweet m8s . <second_speaker> <at> do u hv drake ft jay - z - light up track ? <first_speaker> <at> dawg , my hiphop collection z top noch . ! gt alla drake 's mixtapes & albums + alla jigga 's albums2 . <second_speaker> <at> u got hit stuff ke i 'm sure of ithard as a motherfucker <first_speaker> <at> yea bro , dwnloadd it last week . track 's supa dope . ! !\n",
      "\n",
      "<second_speaker> <at> it was released on th <number> th of january i got it on th <number> th of january !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> please can everyone stop using hipstamatic . please . p . l . e . a . s . e .\n",
      "\n",
      "<second_speaker> <at> thank you friend .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> lol that 's sweet but i think she was humouring you lol xxx <second_speaker> <at> no way . <at> wants me and your jealous lol she said she was flattered which means i 'm in lol xx <first_speaker> <at> <at> lol im sure ! lol however do u want to be ' in ' with her ? lol xxx\n",
      "\n",
      "<second_speaker> <at> i don 't think so . i 'm not sure if i 'm not the only one who doesn 't want to see her . i 'm not sure if i 'm her or not . xx\n",
      "=====\n",
      "Human\n",
      "<first_speaker> 40thingsaboutme <number> i 'm kinda dying to watch harry potter , winnie the pooh and the smurfs $ <second_speaker> <at> ive already seen the winnie the pooh movie . its pretty good\n",
      "\n",
      "<first_speaker> <at> really i 'm def going to see it now\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <url>\n",
      "\n",
      "<second_speaker> <at> that work inspired me to think about nicki minaj . fill but wet .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> ahaha ohh dannq really ? lol dd & i rememeber i stayed up till <number> am on a school night ahaa andd ohh yea me to weekends wenever <second_speaker> <at> lol ive done that so many time during the school weeks . badasses ! <first_speaker> <at> ahaha i know were badass <second_speaker> <at> yeebuddy ! lmao i noticed you wrote that tweet like three times last night p <first_speaker> <at> aha yea cuz i put your username wronq that was a fail aaha dd <second_speaker> <at> epic fail hahah so whats up ? happy easter btw <first_speaker> <at> aha i know xd andd oh yeah happy easter d and i just woke up what about youu ?\n",
      "\n",
      "<second_speaker> <at> just eating a banana lol\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> we are excited to host this month 's cocoaheads meeting at museao on thursday evening ! all <at> you are invited to join in ! <second_speaker> <at> what is it ?\n",
      "\n",
      "<first_speaker> <at> i 'll be there in <number> minutes .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> ahha , noooo ! you don 't need em p\n",
      "\n",
      "<second_speaker> <at> i have so many it doesn 't even matter ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> yeah right ! <second_speaker> be very afraid . * voice echoes * <at> <first_speaker> <at> i have stamina ! the physical one not the line . <second_speaker> oh really . <at> & gt & gt & gt over compensation . much ? & lt & lt & lt <first_speaker> <at> am actually thinking i should buy the ps3 . and a dvd player and abandon the home theatre ! <second_speaker> how will that work exactly ? big screen makes the mind and eyes happy ! <at>\n",
      "\n",
      "<third_speaker> <at> <at> wht 's cup wa ? it 's not cupcake for sure ! hahahahahahah\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> just found out the fellow who i 'm meeting during the week has the best nickname in the world . let the jokes fly ! ! !\n",
      "\n",
      "<second_speaker> <at> yup . i know how it feels .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> its about that time to get back to school <second_speaker> <at> con grates my nigga got to get this second ring <first_speaker> <at> were the black bruh damn <second_speaker> <at> bruh i need one right now too\n",
      "\n",
      "<first_speaker> <at> you need to get a new one\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> my cousin just had a baby girl i 'm sooooo happy <second_speaker> <at> congrats to ur cousin btw heyaaaa wasssapin ? <first_speaker> <at> thank you and soz for the late reply but i 'm good just revising <second_speaker> <at> oh wat u revising for ? <first_speaker> <at> maths , grog , french science <second_speaker> <at> oh kool i like maths ! ! ! ! <first_speaker> <at> are u good at algebra <second_speaker> <at> im alright at it <first_speaker> <at> cool do u wanna come do my exam ? lol i wish <second_speaker> <at> yeh go on bladwat levels are you gettin in maths so far ? <first_speaker> <at> not sure but i have to do a huger paper and it takesthe pisss coz i don 't web get what were doing and i 'm in set <number>\n",
      "\n",
      "<first_speaker> <at> nope , im in year <number> , so it 's gcse . l\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> it 's my birthday in <number> days d\n",
      "\n",
      "<second_speaker> <at> ha ! u a lie ! u thought i wasnt gonna find out u was lyin about ur bday ! i didnt believe u in the first place\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> completely agree . there 's something to be said about the \" local voices \" on the radio . salk just isn 't that .\n",
      "\n",
      "<second_speaker> <at> hi . i live in brazil love the band <at>\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i don 't regret any of the situations that i 've been put in , just the people . <second_speaker> <at> heyyyyyy what 's happenin ? !\n",
      "\n",
      "<third_speaker> <at> <at> she b lost pup\n",
      "=====\n",
      "Human\n",
      "<first_speaker> ireland bowling and fielding definitely looks better than india . wc11 <second_speaker> <at> i am pretty sure that the worst fielding side is either us or pakistan . <third_speaker> <at> <at> hey lets not beat our team down <number> much . we 're still strong contenders for the cup <first_speaker> <at> <at> so are pak , aus , eng , sa . my observation v ireland can not be taken lightly ! we have the best batting line up . wc11 <minor_speaker> <at> <at> <at> our fielding isnt great . but we wouldnt be no . <number> in tests & no . <number> in odis without a good bowling lineup <first_speaker> <at> <at> <at> ranking is cumulative effort ! batters compensated when bowlers & fielders bled and could chase down bigtm ! <third_speaker> <at> <at> <at> hey ! in recent series outside ind its the bowlers who have won us games <minor_speaker> <at> <at> <at> out of the last <number> odi wins , bowlers have won us <number> of them justsaying\n",
      "\n",
      "<minor_speaker> <at> now that 's why he \\ s a tricky customer\n",
      "=====\n",
      "Human\n",
      "<first_speaker> we need lower tuition and we need it now ! !\n",
      "\n",
      "<second_speaker> <at> lol . u at the rally thing ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily btrfamily for btr <second_speaker> <at> can you do <number> for my family too ? <first_speaker> <at> the btrfamily thing ? ? ? <second_speaker> <at> yaha <first_speaker> <at> btrfamily buddy ! ! ! ! ! ! !\n",
      "\n",
      "<second_speaker> <at> can i be in your show <number> day ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> listen , if you thought the social network was a better movie than the king 's speech .\n",
      "\n",
      "<second_speaker> <at> . then you haven 't seen the king 's speech like me -\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> no panties no bra . just skin . \" - - - o _ o\n",
      "\n",
      "<second_speaker> <at> <at> yeah thats the fam . really the only rap cats around here i rock with outside of my crew .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> oh no . imbi market is shutting down in a few months !\n",
      "\n",
      "<second_speaker> <at> the one in the morning behind freeform 's old office and near eric 's studio ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> survived the week without a phone . huhu twitter i miss you !\n",
      "\n",
      "<second_speaker> <at> twitter doesnt want you to watch this <url>\n",
      "=====\n",
      "Human\n",
      "<first_speaker> rt how are you not verified ? ! via <at> good question !\n",
      "\n",
      "<second_speaker> <at> get <at> on the job . she 'll see to it . <at>\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i wonder if hulu is firewalled at work . i need to catch up on glee . <second_speaker> <at> if you get caught watching you can always pass it off as educational and say it 's in a school setting . lol <first_speaker> <at> i like the way you think ! <second_speaker> <at> what day were you at the ellen taping ?\n",
      "\n",
      "<first_speaker> <at> i went to monday 's show but you couldn 't see me\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> zero gravity + my stomach not so good . lol\n",
      "\n",
      "<second_speaker> <at> know the feeling , btw have a daughter who had a heart transplant and follows you totally she s <number> and has md we live in soo mi\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> what 's up ! ! rt <at> smh so hard ! ! ! ! !\n",
      "\n",
      "<second_speaker> <at> <at> aaaahhh both of yall shut it\n",
      "=====\n",
      "Human\n",
      "<first_speaker> why do american forms always ask for a fax number ? is the u . s still stuck in <number> or something ? <second_speaker> <at> the short answer is yes . although i will admit it has been a long time since i 've seen a proper fax machine . <first_speaker> <at> i think my folks still have one plugged in next to the betamax . they enjoy using while drinking sodastream fizzy drinks .\n",
      "\n",
      "<second_speaker> <at> faxes are about as relevant as mimeographs . with fewer purple stains on your new hypercolor shirt mom will still be pissed .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> danger class iv <url> <second_speaker> <at> try not to create an \" black mesa incident \" at your home lab d\n",
      "\n",
      "<first_speaker> <at> i know ! ! ! ! ! ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> i 'm guessing people know this already , but jaejoong is going to hokkaido and yoochun is going to osaka d <second_speaker> <at> i didn 't know alkdj hokkaidooooo <first_speaker> <at> maybe jae will attempt and fail skiing again d <second_speaker> <at> omg he 'd be so pretty & ethereal again with all the pretty snow i hope ! . i know he doesn 't like being called pretty hahaha <first_speaker> <at> i know d i was spazzing so hard when i watched <number> hree voices . replayed his part so many times & gt & gt <second_speaker> <at> i watched it so many times i don 't even - & many of my favourite icons come from his <number> hree voices part a he was so beautiful <first_speaker> <at> lmao * looks at your pic * i can see . i loved when he was on the sled and trying to ski . he 's like a little boy a so cute . <second_speaker> <at> lol i totally forgot that it was from <number> hree voices otl it 's perfect as a wallpaper too ! hahahah . he 's always a little boy <heart> <first_speaker> <at> the wallpaper is so preettyyyyyyy d i love that pic your dp , his expression is so . a i love frapbois wallpapers . <second_speaker> <at> a ahh i miss him sigh . <first_speaker> <at> me too _ _ _ <number> months since i 've seen him d are you going to the sg con ? <second_speaker> <at> it 's been <number> for me . sighs and i have to go , no ? i think i might die if i don 't get to see him . i 'm using up all my savings d <first_speaker> <at> that is so true . if they came to hk , i 'd find some way of going at all costs the last time i went it felt so surreal . <second_speaker> <at> . bb how old are you ? i realise i don 't know d i hope they 'll go there then ! it still feels surreal now .\n",
      "\n",
      "<first_speaker> <at> holles in australia but yet 'm going to pack a og togayyyyyyyyyyyyy . meeting soon ? i 'm channels from\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> aye brah <second_speaker> <at> tyyyyyyyyyyyyyyyyyyyyyyy ! lmaoo <first_speaker> <at> watts up ?\n",
      "\n",
      "<second_speaker> <at> lying your ass . lol how u been ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> yes2av chester ! <url>\n",
      "\n",
      "<second_speaker> <at> that 's massive ! you doing stuff with the y2fv north west people , then ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> for anyone else that does algebra for fun it 's for \" practice \" , but i just use it to create the equations . <url> math <second_speaker> <at> i have a matrix solver on my phone . mostly because when i make up my own matrices they 're impossible . good on you . <first_speaker> <at> do you use driod ? * looks for app for that *\n",
      "\n",
      "<second_speaker> <at> yes , i do .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> tomyam noodle soup is the best , ^ ^ . daebak !\n",
      "\n",
      "<second_speaker> <at> you remind me of chace crawford\n",
      "=====\n",
      "Human\n",
      "<first_speaker> anyone going out tonight ? <second_speaker> <at> trying to\n",
      "\n",
      "<first_speaker> <at> i may depending on the weather and how the thunder nuggets game looks\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i don 't know what to wear tonight . i hate when <at> and i are mistaken for a couple because i 'm to <heart> <cont> <url> <second_speaker> <at> you have this bad habit to go everywhere like you have to go to a ball ! xd xd <first_speaker> <at> it 's not true , i just hate to go out as if i was to go to classes when i get depressed and want to do nothing . <second_speaker> <at> there 's a difference , a golden mean between this and going dressed like a lady who 's going to a ball xd xd\n",
      "\n",
      "<first_speaker> <at> i 'll take it as a compliment u _ _ u\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> kinda excited jules ! ! <second_speaker> <at> it 's totally addictive - <first_speaker> <at> i know ! ! see all that bullsh * t with amir khan ? ! ? did you follow him ?\n",
      "\n",
      "<second_speaker> <at> he definitely has twitter tom cheryl 's legend x\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> good evening everyone . how is everyone doing ? <second_speaker> <at> good afternooon ! i am ready for spring . not looking forward to the snow & mix tomorrow ! <first_speaker> <at> who is . since it 's officially spring around <number> pm tonight it should not be snowing tomorrow . mother nature should know the rules\n",
      "\n",
      "<second_speaker> <at> thank god ! oh , and thank you .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> ok , for all who would like to continue hosting with us on a shared cloud would you prefer one plan fits all or current plans migrated ?\n",
      "\n",
      "<second_speaker> <at> good idea !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> your show was amazing ! wish i was sitting closer to volunteer ! ahahah love you ! ! & lt <number> <second_speaker> <at> he is the nicest person ever my dad works with him and i have met him many times <first_speaker> <at> ? <second_speaker> <at> cris angel <first_speaker> <at> ohh ! he is awesome he is really nice ! <second_speaker> <at> ik he singed my laptop , his book , and a boblehead for me lol his magic set is my dads product <first_speaker> <at> thats soo cool ! im jealous now xd\n",
      "\n",
      "<second_speaker> <at> hahaha he is kinda scary though lol he wears weird stuff\n",
      "=====\n",
      "Human\n",
      "<first_speaker> i still haven 't found me nothing to wear for sunday ugh this sucks i hate last min shopping for an outfit that has to be overthetopbad . <second_speaker> <at> we need to go shopping together ! i have to fun jeans for sunday ! <first_speaker> <at> me megan and roc went today ! do you work tomorrow ? <second_speaker> <at> i work <number> - <number> so maybe after ?\n",
      "\n",
      "<first_speaker> <at> me to , but you said you have to wear jeans sunday ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> chowing down some tacos while watching the biggest loser . it 's one of my hobbies . <second_speaker> <at> the other day <at> and i were watching biggestloser whilst eating a large pizza classic\n",
      "\n",
      "<first_speaker> <at> <at> hahahaha ! maybe one day we 'll have a reunion together on the show of the biggest loser . lol\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> omg i can 't believe that exactly a week ago today my dream to meet <at> came true ! it was the best day of my life <heart> neversaynever <second_speaker> <at> omg me to . today last week i met him aswell and he done the best thing ever ! <first_speaker> <at> omg at the premiere right ? d what did he do ? ? <second_speaker> <at> yeah i went to the premiere ! he randomly gave me tickets to the screening . and then he suprised us in the cinema ! lol <first_speaker> <at> omg ! ! u so lucky ! ! i waited til he came out the cinema cos he came out half way through it , then i saw him cos he came over . <second_speaker> <at> yeah omg you must of been with my two other friends cuz they met him the way you did . lol apparently he called thembeautiful <first_speaker> <at> yeah loads of ppl left cos the bodyguards kept lying saying how he wasn 't gonna come back bt i was like noway ! lol <second_speaker> <at> lol yeah never trust body guards their just loosers lol <first_speaker> <at> lol yeah & one of them was so rude about justin he was like how can a <number> year old make a movie bout his life that 's pathetic .\n",
      "\n",
      "<second_speaker> <at> well now he 's gone liverpool are doing well anyway , so he can suck out -\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> who is micha ?\n",
      "\n",
      "<second_speaker> <at> ? ? you mean michael ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> anyone looking for more business ? are you a norwich based business ? contact me for an invitation to our networking group ! ! ! ! ! <second_speaker> <at> can you send me over an invitation for your networking group for <at> thanks danny\n",
      "\n",
      "<first_speaker> <at> is amy your new one ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> nowfollowing <second_speaker> <at> following back\n",
      "\n",
      "<first_speaker> <at> thanks for the follow . what 's up\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <at> what made you want to do that n i 'm cutting my hair <second_speaker> <at> murph asked me if i wanted to do it . so i 'm going now . & what u mean ur cutting ur hair ? <first_speaker> <at> wen are you goin to do it and in a bob <second_speaker> <at> i 'm getting it now <first_speaker> <at> wait huh now ? ? ? ? what i should have been there\n",
      "\n",
      "<second_speaker> <at> lol it 's a shot girl . i 'm already gone . & i can 't believe u gone cut ur hair .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> <second_speaker> <at>\n",
      "\n",
      "<third_speaker> <at> <at> yeah same here i didnt got what he meant by openness tourist\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> over beeb lunch i try to remember old address and agonize over possibility days filming might be a wash out <url>\n",
      "\n",
      "<second_speaker> <at> please tell me it 's fixed so we are on some choices across rumours . buy radio bacon and d . v . asb\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(500, 600):\n",
    "    print('Human' if frame.iloc[i]['human-generated'] == 1 else 'Generated')\n",
    "    print(frame.iloc[i].context)\n",
    "    print('')\n",
    "    print(frame.iloc[i].response)\n",
    "    print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # treat apostrophes as one word\n",
    "# apostrophe_tokens = set()\n",
    "# for context, response in zip(frame['context'], frame['response']):\n",
    "#     response_tokens = response.split(' ')\n",
    "#     context_tokens = context.split(' ')\n",
    "#     for token in response_tokens:\n",
    "#         if \"'\" in token:\n",
    "#             apostrophe_tokens.add(token)\n",
    "#     for token in context_tokens:\n",
    "#         if \"'\" in token:\n",
    "#             apostrophe_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apostrophe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# message = '<second_speaker> <at> faxes are about as relevant as mimeographs . with fewer purple stains on your new hypercolor shirt mom will still be pissed .'\n",
    "# tokens = message.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_list = []\n",
    "all_labels_list = []\n",
    "short_frame = frame[['context', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f extract_features extract_features(short_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_counters(frame):\n",
    "    global human_counter, generated_counter\n",
    "    labels = frame['human-generated'].values\n",
    "    labels_sum = labels.sum()\n",
    "    human_counter += labels_sum\n",
    "    generated_counter += frame.shape[0] - labels_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 5s\tChunk 10 over: 9s\tChunk 15 over: 13s\tChunk 20 over: 16s\tChunk 25 over: 21s\tChunk 30 over: 25s\tChunk 35 over: 29s\tChunk 40 over: 33s\tChunk 45 over: 37s\tChunk 50 over: 41s\tChunk 55 over: 45s\tChunk 60 over: 49s\tChunk 65 over: 53s\tChunk 70 over: 57s\tChunk 75 over: 61s\tChunk 80 over: 65s\tChunk 85 over: 68s\tChunk 90 over: 72s\tChunk 95 over: 76s\tChunk 100 over: 81s\tChunk 105 over: 85s\tChunk 110 over: 89s\tChunk 115 over: 92s\tChunk 120 over: 97s\tChunk 125 over: 101s\tChunk 130 over: 111s\tChunk 135 over: 124s\tChunk 140 over: 138s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "human_counter = 0\n",
    "generated_counter = 0\n",
    "run(write_counters, full_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3595488 3595488\n"
     ]
    }
   ],
   "source": [
    "print(human_counter, generated_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tags(frame):\n",
    "    responses_frame = frame['response']\n",
    "    \n",
    "    token_lists = responses_frame.str.split(' ').values\n",
    "    labels = frame['human-generated'].values\n",
    "    \n",
    "    for i, token_list in enumerate(token_lists):\n",
    "        tag_list = list(filter(lambda token: token[0] == '<' and token[-1] == '>', token_list))\n",
    "        tag_set = set(tag_list)\n",
    "        for tag in tag_set:\n",
    "            tag_count = tag_list.count(tag)\n",
    "            if labels[i] == 0:\n",
    "                tag_dict_generated[tag]['occ'] += 1\n",
    "                tag_dict_generated[tag][tag_count] += 1\n",
    "                tag_dict_generated[tag]['sum'] += tag_count\n",
    "            else:\n",
    "                tag_dict_human[tag]['occ'] += 1\n",
    "                tag_dict_human[tag][tag_count] += 1\n",
    "                tag_dict_human[tag]['sum'] += tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 18s\tChunk 10 over: 28s\tChunk 15 over: 34s\tChunk 20 over: 40s\tChunk 25 over: 46s\tChunk 30 over: 52s\tChunk 35 over: 59s\tChunk 40 over: 65s\tChunk 45 over: 71s\tChunk 50 over: 77s\tChunk 55 over: 84s\tChunk 60 over: 90s\tChunk 65 over: 96s\tChunk 70 over: 102s\tChunk 75 over: 108s\tChunk 80 over: 114s\tChunk 85 over: 120s\tChunk 90 over: 126s\tChunk 95 over: 133s\tChunk 100 over: 148s\tChunk 105 over: 164s\tChunk 110 over: 179s\tChunk 115 over: 192s\tChunk 120 over: 205s\tChunk 125 over: 219s\tChunk 130 over: 233s\tChunk 135 over: 246s\tChunk 140 over: 259s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "tag_dict_human = defaultdict(lambda: defaultdict(int))\n",
    "tag_dict_generated = defaultdict(lambda: defaultdict(int))\n",
    "run(write_tags, full_train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tag statistics saved in analysis_results/tag_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for token_to_count in tokens_to_count:\n",
    "#     print('trying ' + token_to_count)\n",
    "#     model = train(train_filename)\n",
    "#     auc = estimate_auc(model)\n",
    "#     print('AUC ' + str(round(auc, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_punct(frame):\n",
    "    responses_frame = frame['response']\n",
    "    \n",
    "    token_lists = responses_frame.str.split(' ').values\n",
    "    labels = frame['human-generated'].values\n",
    "    \n",
    "    for i, token_list in enumerate(token_lists):\n",
    "        punct_list = list(filter(lambda token: len(token) == 1 and token in string.punctuation, token_list))\n",
    "        punct_set = set(punct_list)\n",
    "        for punct in punct_set:\n",
    "            punct_count = punct_list.count(punct)\n",
    "            if labels[i] == 0:\n",
    "                punct_dict_generated[punct]['occ'] += 1\n",
    "                punct_dict_generated[punct][punct_count] += 1\n",
    "                punct_dict_generated[punct]['sum'] += punct_count\n",
    "            else:\n",
    "                punct_dict_human[punct]['occ'] += 1\n",
    "                punct_dict_human[punct][punct_count] += 1\n",
    "                punct_dict_human[punct]['sum'] += punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 5s\tChunk 10 over: 12s\tChunk 15 over: 19s\tChunk 20 over: 25s\tChunk 25 over: 31s\tChunk 30 over: 38s\tChunk 35 over: 45s\tChunk 40 over: 52s\tChunk 45 over: 58s\tChunk 50 over: 65s\tChunk 55 over: 71s\tChunk 60 over: 77s\tChunk 65 over: 82s\tChunk 70 over: 89s\tChunk 75 over: 95s\tChunk 80 over: 101s\tChunk 85 over: 107s\tChunk 90 over: 113s\tChunk 95 over: 119s\tChunk 100 over: 125s\tChunk 105 over: 131s\tChunk 110 over: 140s\tChunk 115 over: 156s\tChunk 120 over: 167s\tChunk 125 over: 173s\tChunk 130 over: 179s\tChunk 135 over: 185s\tChunk 140 over: 191s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "punct_dict_human = defaultdict(lambda: defaultdict(int))\n",
    "punct_dict_generated = defaultdict(lambda: defaultdict(int))\n",
    "run(write_punct, full_train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation statistics saved in analysis_results/punc_analysis.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_features_labels(frame): # append features and labels of current frame to a general list\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    labels = frame['human-generated'].values.tolist()\n",
    "    \n",
    "    all_features_list.extend(features) # actually more efficient than numpy append\n",
    "    all_labels_list.extend(labels)\n",
    "    \n",
    "#     print('current len(all_labels_list): ' + str(len(all_labels_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_all_features_labels(filename):\n",
    "    global all_features_list, all_labels_list\n",
    "    \n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    run(write_features_labels, filename)\n",
    "    all_features = np.array(all_features_list)\n",
    "    all_labels = np.array(all_labels_list)\n",
    "    \n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eval_data():\n",
    "    frame = first_frame(eval_filename)\n",
    "    \n",
    "    short_frame = frame[['context', 'response']]\n",
    "    features = np.array(extract_features(short_frame))\n",
    "    \n",
    "    truth = frame['human-generated'].values\n",
    "    \n",
    "    return (features, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rounded_auc(pred_param, labels_param): # custom auc with rounding for correct early stopping in xgboost\n",
    "    pred = 1.0 / (1.0 + np.exp(-pred_param))\n",
    "    labels = labels_param.get_label()\n",
    "    score = roc_auc_score(labels, pred)\n",
    "    return ('auc', round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(all_features, all_labels, eval_features, eval_labels, early_stopping_rounds = 3):\n",
    "    model = XGBClassifier(max_depth = 7, n_estimators = 200)\n",
    "    model = model.fit(all_features, all_labels, eval_set = [(eval_features, eval_labels)],\n",
    "                      eval_metric = rounded_auc, early_stopping_rounds = early_stopping_rounds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast AUC evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_auc(model, eval_features, eval_labels): # AUC on eval formed from train\n",
    "    pred = model.predict(eval_features)\n",
    "    return roc_auc_score(eval_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current len(all_labels_list): 50000\n",
      "Chunk 0 over: 77s\tcurrent len(all_labels_list): 100000\n",
      "current len(all_labels_list): 150000\n",
      "current len(all_labels_list): 200000\n",
      "current len(all_labels_list): 250000\n",
      "current len(all_labels_list): 300000\n",
      "Chunk 5 over: 474s\tcurrent len(all_labels_list): 350000\n",
      "current len(all_labels_list): 400000\n",
      "current len(all_labels_list): 450000\n",
      "current len(all_labels_list): 500000\n",
      "current len(all_labels_list): 550000\n",
      "Chunk 10 over: 885s\tcurrent len(all_labels_list): 600000\n",
      "current len(all_labels_list): 650000\n",
      "current len(all_labels_list): 700000\n",
      "\n",
      "14 chunks overall\n"
     ]
    }
   ],
   "source": [
    "all_features, all_labels = extract_all_features_labels(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(all_features, open('all_features.pickle.dat', 'wb'))\n",
    "all_features = pickle.load(open('all_features.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(all_labels, open('all_labels.pickle.dat', 'wb'))\n",
    "all_labels = pickle.load(open('all_labels.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_features, eval_labels = get_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.711\n",
      "Will train until validation_0-auc hasn't improved in 3 rounds.\n",
      "[1]\tvalidation_0-auc:0.72\n",
      "[2]\tvalidation_0-auc:0.721\n",
      "[3]\tvalidation_0-auc:0.725\n",
      "[4]\tvalidation_0-auc:0.727\n",
      "[5]\tvalidation_0-auc:0.728\n",
      "[6]\tvalidation_0-auc:0.73\n",
      "[7]\tvalidation_0-auc:0.733\n",
      "[8]\tvalidation_0-auc:0.734\n",
      "[9]\tvalidation_0-auc:0.736\n",
      "[10]\tvalidation_0-auc:0.737\n",
      "[11]\tvalidation_0-auc:0.739\n",
      "[12]\tvalidation_0-auc:0.74\n",
      "[13]\tvalidation_0-auc:0.741\n",
      "[14]\tvalidation_0-auc:0.742\n",
      "[15]\tvalidation_0-auc:0.742\n",
      "[16]\tvalidation_0-auc:0.743\n",
      "[17]\tvalidation_0-auc:0.744\n",
      "[18]\tvalidation_0-auc:0.744\n",
      "[19]\tvalidation_0-auc:0.745\n",
      "[20]\tvalidation_0-auc:0.746\n",
      "[21]\tvalidation_0-auc:0.747\n",
      "[22]\tvalidation_0-auc:0.748\n",
      "[23]\tvalidation_0-auc:0.748\n",
      "[24]\tvalidation_0-auc:0.749\n",
      "[25]\tvalidation_0-auc:0.75\n",
      "[26]\tvalidation_0-auc:0.75\n",
      "[27]\tvalidation_0-auc:0.75\n",
      "[28]\tvalidation_0-auc:0.751\n",
      "[29]\tvalidation_0-auc:0.751\n",
      "[30]\tvalidation_0-auc:0.752\n",
      "[31]\tvalidation_0-auc:0.752\n",
      "[32]\tvalidation_0-auc:0.753\n",
      "[33]\tvalidation_0-auc:0.753\n",
      "[34]\tvalidation_0-auc:0.753\n",
      "[35]\tvalidation_0-auc:0.754\n",
      "[36]\tvalidation_0-auc:0.754\n",
      "[37]\tvalidation_0-auc:0.754\n",
      "[38]\tvalidation_0-auc:0.755\n",
      "[39]\tvalidation_0-auc:0.755\n",
      "[40]\tvalidation_0-auc:0.756\n",
      "[41]\tvalidation_0-auc:0.756\n",
      "[42]\tvalidation_0-auc:0.757\n",
      "[43]\tvalidation_0-auc:0.757\n",
      "[44]\tvalidation_0-auc:0.757\n",
      "[45]\tvalidation_0-auc:0.758\n",
      "[46]\tvalidation_0-auc:0.758\n",
      "[47]\tvalidation_0-auc:0.758\n",
      "[48]\tvalidation_0-auc:0.759\n",
      "[49]\tvalidation_0-auc:0.759\n",
      "[50]\tvalidation_0-auc:0.76\n",
      "[51]\tvalidation_0-auc:0.76\n",
      "[52]\tvalidation_0-auc:0.761\n",
      "[53]\tvalidation_0-auc:0.761\n",
      "[54]\tvalidation_0-auc:0.761\n",
      "[55]\tvalidation_0-auc:0.761\n",
      "Stopping. Best iteration:\n",
      "[52]\tvalidation_0-auc:0.761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(all_features, all_labels, eval_features, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open('9_syntactical_features.pickle.dat', 'wb'))\n",
    "# model = pickle.load(open('.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.683648918967105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_auc(model, eval_features, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10119048,  0.01785714,  0.02734375,  0.01171875,  0.04464286,\n",
       "        0.02641369,  0.01971726,  0.01841518,  0.01897321,  0.01134673,\n",
       "        0.00985863,  0.01153274,  0.01153274,  0.00855655,  0.01785714,\n",
       "        0.0109747 ,  0.02176339,  0.01655506,  0.01469494,  0.03627232,\n",
       "        0.02511161,  0.02548363,  0.03869048,  0.00409226,  0.01227679,\n",
       "        0.01785714,  0.02604167,  0.0593378 ,  0.06194196,  0.00483631,\n",
       "        0.        ,  0.00874256,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.01636905,  0.0046503 ,  0.00427827,  0.01134673,\n",
       "        0.00018601,  0.00074405,  0.01488095,  0.01171875,  0.00111607,\n",
       "        0.0014881 ,  0.        ,  0.00167411,  0.0249256 ,  0.00111607,\n",
       "        0.        ,  0.00818452,  0.00037202,  0.00427827,  0.01915923,\n",
       "        0.0031622 ,  0.01357887,  0.        ,  0.00055804,  0.00093006,\n",
       "        0.00018601,  0.01748512,  0.00037202,  0.01227679,  0.00483631,\n",
       "        0.0187872 ,  0.00223214,  0.03943452,  0.00446429,  0.00223214,\n",
       "        0.00799851,  0.        ,  0.00334821,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: feature engineering\n",
    "implement label bigrams from syntactical analysis (45*45 features, needs sparse matrix)\n",
    "(http://www.nltk.org/book/ch05.html)\n",
    "implement bag of words and word n-grams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: tuning\n",
    "min_child_weight, gamma, max_depth => model complexity\n",
    "n_estimators\n",
    "using grid search like hyperopt\n",
    "https://www.dataiku.com/learn/guide/code/python/advanced-xgboost-tuning.html\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: other\n",
    "store features in bytes, train on a larger fraction of the dataset\n",
    "initialize data inside for the functions to run and make them return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_ids(frame):\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    ids = frame['id'].values.tolist()\n",
    "    \n",
    "    test_features_list.extend(features) # actually more efficient than numpy append()\n",
    "    test_ids_list.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 80s\tChunk 5 over: 472s\t"
     ]
    }
   ],
   "source": [
    "test_features_list = []\n",
    "test_ids_list = []\n",
    "run(extract_features_ids, test_filename)\n",
    "test_features = np.array(test_features_list)\n",
    "test_ids = np.array(test_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.concatenate(([test_ids], [predicted_labels]), axis = 0).T, columns = ['id', 'human-generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

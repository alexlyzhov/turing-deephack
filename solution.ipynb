{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format\n",
    "===\n",
    "\n",
    "The data itself is already pre-processed (`<s>, </s>` tags, `<unk>` tag, etc.). The punctuation is tokenized (one symbol = one token, no word-punctuation merged tokens). The is only one space symbol between every two adjacent tokens.\n",
    "\n",
    "UPD: In the data there are common combination \"@@ \". This is because the data has been preprocessed with BPE encoding (see: https://arxiv.org/abs/1508.07909 and https://github.com/rsennrich/subword-nmt) in order to reduce the vocabulary size.\n",
    "\n",
    "In order to properly print a message you should make sure to do the following in Python:\n",
    "\n",
    "[your string message here].replace(‘@@ ‘, ‘’)\n",
    "\n",
    "NOTE: replace `@@space` by nothing. Don’t forget the [space]!\n",
    "\n",
    "Broadly speaking, BPE encoding will split words into the most common n-gram to reduce the vocabulary size. The ‘@@ ’ you see are tokens to indicate there was a split. Thus to print the actual word you should replace all occurrences of '@@' to nothing.\n",
    "\n",
    "\n",
    "Data fields are separated by one tab character.\n",
    "\n",
    "    context - context phrase(s) for response, always human generated\n",
    "    response - one phrase or a few phrases, may be from different speakers\n",
    "    human-generated - flag if the response is generated by human\n",
    "\n",
    "Data header: 'id\\tcontext\\tresponse\\thuman-generated\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission format\n",
    "===\n",
    "\n",
    "ROC AUC score.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "id,human-generated\n",
    "\n",
    "1,1\n",
    "\n",
    "8,0\n",
    "\n",
    "9,1\n",
    "\n",
    "10,1\n",
    "\n",
    "We expect the solution file to have 524,342 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # download corpora->words, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 50000\n",
    "train_filename = 'data/sampled_train.txt'\n",
    "full_train_filename = 'data/train.txt'\n",
    "test_filename = 'data/test.txt'\n",
    "eval_filename = 'data/eval.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(function, filename, print_every = 5, chunks = None):\n",
    "    start = time()\n",
    "    for i, frame in enumerate(pd.read_csv(filename, chunksize = chunksize, delimiter = '\\t')):\n",
    "        frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "        \n",
    "        function(frame)\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print('Chunk ' + str(i) + ' over: ' + str(round(time() - start)) + 's', end = '\\t')\n",
    "        if chunks is not None:\n",
    "            if i >= chunks - 1:\n",
    "                break\n",
    "    print('\\n' + str(i + 1) + ' chunks overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(frame):\n",
    "    frame_features = []\n",
    "    \n",
    "#     for index, row in frame.iterrows():\n",
    "#     for row in frame.itertuples(): # requires 'dot access' to columns\n",
    "#     zip is faster than itertuples and a whole lot faster than iterrows\n",
    "    for context, response in zip(frame['context'], frame['response']):\n",
    "        features = []\n",
    "        \n",
    "        # context length and number of tokens in context are usually useless\n",
    "        \n",
    "        response_tokens = response.split(' ')\n",
    "        context_tokens = context.split(' ')\n",
    "    \n",
    "        # feature: response length\n",
    "        features.append(float(len(response)))\n",
    "\n",
    "        # feature: number of tokens in the response\n",
    "        features.append(float(len(response_tokens)))\n",
    "        \n",
    "        # features: number of words of length 1, 2, ..., 5\n",
    "        # counters of length 6+ don't seem to have importance\n",
    "        lens = Counter(map(len, response_tokens))\n",
    "        for i in range(5):\n",
    "            features.append(float(lens[i + 1]))\n",
    "        \n",
    "        # features: number of specific tokens from the list in the response\n",
    "        tokens_to_count = ['<at>', '<number>', '!', '.']\n",
    "        for token_to_count in tokens_to_count:\n",
    "            features.append(float(response_tokens.count(token_to_count)))\n",
    "            \n",
    "        # features: includes apostrophes, is composed of english letters, is in vocabulary, is a stopword\n",
    "        # for context and response\n",
    "        apostrophe_counter = 0\n",
    "        alpha_counter = 0\n",
    "        vocab_counter = 0\n",
    "        stop_counter = 0\n",
    "        counters = [0] * 8\n",
    "        for token in response_tokens:\n",
    "            if \"'\" in token:\n",
    "                counters[0] += 1\n",
    "            if token.isalpha():\n",
    "                counters[1] += 1\n",
    "            if token in vocab:\n",
    "                counters[2] += 1\n",
    "            if token in stopwords:\n",
    "                counters[3] += 1\n",
    "        for token in context_tokens:\n",
    "            if \"'\" in token:\n",
    "                counters[4] += 1\n",
    "            if token.isalpha():\n",
    "                counters[5] += 1\n",
    "            if token in vocab:\n",
    "                counters[6] += 1\n",
    "            if token in stopwords:\n",
    "                counters[7] += 1\n",
    "        for counter in counters:\n",
    "            features.append(float(counter))\n",
    "            \n",
    "            \n",
    "        all_tokens = set(context_tokens + response_tokens)\n",
    "        shared_tokens = set(context_tokens).intersection(response_tokens)\n",
    "\n",
    "        # feature: number of shared tokens between context and response\n",
    "        features.append(float(len(shared_tokens)))\n",
    "\n",
    "        # feature: sum of abs diffs in token shares\n",
    "        diff = 0\n",
    "        for token in all_tokens:\n",
    "            token_stat = abs(float(context_tokens.count(token)) / len(context_tokens) -\n",
    "                         float(response_tokens.count(token)) / len(response_tokens))\n",
    "            diff += token_stat\n",
    "        features.append(diff)\n",
    "        \n",
    "        frame_features.append(features)\n",
    "\n",
    "    return frame_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_frame(print_filename):\n",
    "    for frame in pd.read_csv(print_filename, chunksize = chunksize, delimiter = '\\t'):\n",
    "        frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "        break\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = first_frame(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "<first_speaker> sounds like many of my aie peeps had a crappy day ! hugs to all\n",
      "\n",
      "<second_speaker> <at> i 'll post sex so might fall asleep . have a good nights sleep too i can do it all day * sunbathing on the sofa instead screams *\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> posting a video soon about the new album . excited nervous to see what you kittens think . any guesses to what the album name is ?\n",
      "\n",
      "<second_speaker> <at> what is it ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> so there 's a mouse at work .\n",
      "\n",
      "<second_speaker> <at> wtf it betta b the <number> w <number> legs & a ponytail ! not <number> legs w a long tail ! coughing umm yea can 't make it in <number> day !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> in new york concrete jungle wet dream tomato . <second_speaker> <at> go to bed la boy .\n",
      "\n",
      "<first_speaker> <at> shit seems like that many drinks are in the gym ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> official event hashtag is soslam <at> <at>\n",
      "\n",
      "<second_speaker> <at> <at> i 'm all over it ! at dulles , making my way !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> exercise makes me soooo hungry . seems counterproductive to me <number>\n",
      "\n",
      "<second_speaker> <at> i especially get that with swimming - makes me absolutely ravenous ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> isn 't it lovely to wake up an ikey this morning <at> ? ?\n",
      "\n",
      "<second_speaker> <at> your thoughts on the game and win ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> andy sixx is the hottest emo guy \" & lt - - poser saying six is with ones x & <at> is not emo or gothic . he s just himself . d\n",
      "\n",
      "<second_speaker> <at> evening - . how 's you ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> even tho i pay $ 10 \\ month for black tie warranty on my evo best buy says i may have to wait <number> days ? ! ? <at> this is unacceptable nophone <second_speaker> <at> sorry to hear about your phone . email us your info to twitter <at> . com and someone from my team will get back <number> u .\n",
      "\n",
      "<first_speaker> <at> thank you for the response ! just sent the email\n",
      "=====\n",
      "Human\n",
      "<first_speaker> making the album part <number> today i was writing lyrics for a great summer song ! i wrote <number> verses , the chorus and . <url>\n",
      "\n",
      "<second_speaker> <at> try it out on your lv audience !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> listening to elvis costello 's little triggers and i forgot just how good it is . elviscostello can do no wrong in my book . <second_speaker> <at> amen sister <first_speaker> <at> seriously best song writer ever and his voice just makes the heart ache so good lol elviscostello <second_speaker> <at> doesn 't it make u feel about <number> % smarter than everyone else knowing that ? it 's so easy to cite , say , dylan , but ec . brilliant <first_speaker> <at> right behind him though is mr tom waits , such a storyteller of humor & sentimentality . tomwaits martha <second_speaker> <at> omg . have you been reading my diary ? you 're much older too ! ! ! ! ! mylonglongsister ! ! ! <first_speaker> <at> born in the <number> s product of the <number> s haha but i dress like i am still <number> and that 's the age i 'm stickin with lol ! ! <second_speaker> <at> gonna push my luck here ? ? an idiot abroad ? ? ? ? rowan atkinson ? ? ? tff ? ? ? nick lowe \" ? ? ? mike leigh ? ? <first_speaker> <at> i follow <at> . and let 's not forget the kiwis neil finn tim finn split enz and flight if the conchords haha ! !\n",
      "\n",
      "<second_speaker> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at>\n",
      "=====\n",
      "Human\n",
      "<first_speaker> if y 'all reproduce , you will make the most horrible people on the planet . i can 't even wrap my head around it . <second_speaker> <at> you best not be talkin bout me !\n",
      "\n",
      "<first_speaker> <at> haha you know i 'm not .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> thanks babe dddd here 's mine - <url> ddd\n",
      "\n",
      "<second_speaker> <at> your welcome -\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> its too easy to beef . i takes a man to be mature about certain situations\n",
      "\n",
      "<second_speaker> <at> hahaha follow me back\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> just clicked on ur profile to see pic your son took a bit bigger then saw ur \" about me or whatever it 's called \" love it ! ! lol <second_speaker> <at> i just want to make sure no one 's under any illusions before they start talking to me <first_speaker> <at> i think it 's great - <second_speaker> <at> thanks . how 's your bank holiday weekend ? <first_speaker> <at> good thanks . weather great . we r not doing anything special just plodding along . you ? <second_speaker> <at> yeah friday saturday were good , the boy had a bit of a hissy fit day today which sucked , but tomorrow 's another day <first_speaker> <at> how old is he ? i played footie v mine <number> day . parents v the u6s . got a big bruise from him running into me , kicking my calf - <second_speaker> <at> he 's <number> . we were playing kinect sports , his attention span isn 't really up to going and having a full game of football yet <first_speaker> <at> i was glad he was our <number> rd then as it made us more relaxed about it . i just let him lie there - until he gave up <second_speaker> <at> i always get on the verge of losing my rag , then think he 's a good lad and it 's only natural . makes it better <first_speaker> <at> it is natural as it is for parents to lose it at times , i shout but they very rarely take notice . <second_speaker> <at> i 've tried to be more calm . i 'm a grumpy , short - tempered bastard , but it wouldn 't be good for him to see me like that . hi twitter ! <first_speaker> <at> oh , i have little patience and like things my way . get angry quickly , passes quickly - they do get <number> see it as it 's me .\n",
      "\n",
      "<first_speaker> <at> called ' vante ' and he 's haaaaaaaawt . lmao . idk , just saw that on my tl ' . i didn 't know austin was <number> ? ! dead .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> well . who knew is my favourite song . beside sober <second_speaker> <at> and every time i heard fuckin ' perfect i don 't know why but i really cry <at> writes really good songs d\n",
      "\n",
      "<first_speaker> <at> i love it ! ! ! ! ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> he can 't whoop my ass rt <at> seriously ? imma tell yo bro what u jus said rt <at> shut that ish up <cont> <url>\n",
      "\n",
      "<second_speaker> <at> lol i 'm just saying\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> no one tweets in the morning wtf teamfollowback <second_speaker> <at> hahahah shutup ! ! ! ! ! <first_speaker> <at> lol i know i 'm bored . that 's crazy cuz i was just thinking about tweeting you teamfollowback\n",
      "\n",
      "<second_speaker> <at> lol ! ! ! ! ! ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> rt <at> omds so what 's the flipping score ? kmt & lt don 't worry go and make me a sandwich\n",
      "\n",
      "<second_speaker> <at> rio did you really just tweet me this ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> permanent marker on a white board . thanks <at> ! <url>\n",
      "\n",
      "<second_speaker> <at> you 're welcome . i 'm a huge fan of you .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> rt <at> <at> muah ! ! ! just got back in hope your day is going well xoxoo <second_speaker> <at> so far so good . how 's things there ? <first_speaker> <at> not bad at all checking things quick online and headed to bed <second_speaker> <at> i just finished lunch . <first_speaker> <at> nice ! ! so what was in the menu <second_speaker> <at> chinese\n",
      "\n",
      "<first_speaker> <at> nice ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> patient said \" daaaamn you done get fat , boy ! \" def gonna run today . asshole <second_speaker> <at> lmfao how kind of him\n",
      "\n",
      "<first_speaker> <at> def a female patient . she was like \" you a pretty boy , go get that weight back off\n",
      "=====\n",
      "Human\n",
      "<first_speaker> new video up with <at> going over the surgeon head . check it out ! <url> <at> <at> <at> <second_speaker> <at> could you give me your honest opinion . are the assault ag more protective than the g22 ag 's ? thank you <first_speaker> <at> i prefer the assault ags and that is what i currently wear <second_speaker> <at> thanks . just sold my shakedowns which i didnt like whatso ever , buying the assaults today .\n",
      "\n",
      "<first_speaker> awesome way to start my day ! rt <at> thanks . just sold my shakedowns which i didnt like whatso ever , buying the assaults today\n",
      "=====\n",
      "Human\n",
      "<first_speaker> 40 in the <at> for the manu game ! ! ! nufc <second_speaker> <at> when is the big game ? ? - <first_speaker> <at> what big game ? <second_speaker> <at> manu v nufc ? ? <first_speaker> <at> <number> th you going ? <second_speaker> <at> i 'd love to but with it being midweek not sure how much work i 've on . also ling way to travel . <first_speaker> <at> where do you stay ? <second_speaker> <at> live just outside bournemouth in dorset . <first_speaker> <at> wow that is a long way to travel ! take it you don 't make many games ? <second_speaker> <at> unfortunately not . partly money and travelling always been the problem . have to stick with pub and pint instead . <first_speaker> <at> well least its on sky for ya , i don 't expect us to get anything from it tho jajajaja <second_speaker> <at> yeah , i was hoping it would . always like an excuse to escape to the pub .\n",
      "\n",
      "<first_speaker> <at> your next <number> games are on sky cl quarter final , facup semi then the big one against the mighty nufc jajajaja pubcallingjames\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> heey <at> white and pointy p x haha . fashion statment of the year ! x <second_speaker> <at> ohh dear . it 's a something statement of the year xx <first_speaker> <at> pahahaha i want some p x <second_speaker> <at> enjoy yourself - you can have my share s l xx <first_speaker> <at> my share of what ? xxx <second_speaker> <at> the white pointy stuff l x <first_speaker> <at> haha l x yeah p x <second_speaker> <at> lol l oooohhh . i have a new obsession zayn malik . he 's so bloody gorgeous & lt <number> xx <first_speaker> <at> i havent quite grown the obsestion but i agree he is gorgeous & lt <number> x <second_speaker> <at> he 's my background on my desktop . it changes every half an hour . i just drool pathetically over it ' l xx <first_speaker> <at> haha p x im gunna change mine to rlo now x <second_speaker> <at> lol . they were my original one l i have <number> pictures of him ? obsessed ? l xx <first_speaker> <at> im proud of u lou not so obsesed with rlo . haha just like robyn was pround when i was over jb x l x <second_speaker> <at> but now im obsessed with rlo and zayn ! help ! ! l xx <first_speaker> <at> i love how ur obssesed with rlo and zayn not rlo and <number> d . l xx i used to be obsesed with <number> d and rlo <second_speaker> <at> * <number> more times ! x <first_speaker> <at> haha u no u gotta obsestion when all u think and tallk about is them l true hotness they r the same x <second_speaker> <at> they 're not the same - they 're very different , but they 're just equally as hot ! ! l xx <first_speaker> <at> yeah i mean the same hotness l & lt <number> im just watching videos of <number> d p x <second_speaker> <at> i think i must have seen every one that included zayn one million times l x <first_speaker> <at> haha p gotta love ' em x <second_speaker> <at> i reaally do ! l xx <first_speaker> <at> p xxx <second_speaker> <at> zayn or niall ? x <first_speaker> <at> both x <second_speaker> <at> pahaha ! if you had to choose one ? x\n",
      "\n",
      "<first_speaker> <at> but i want to d xxx\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> i love this picture of demi from tonight . <url> <second_speaker> <at> nice picture ! , where did you find it ?\n",
      "\n",
      "<first_speaker> <at> i don 't know , i just found it on youtube . i love it .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> everyone when silent when i was down with problems . haissh\n",
      "\n",
      "<second_speaker> <at> whaaat ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> <at> yall got see how ikea dresses now that i told her to stay out of my closet cthu barefeet <second_speaker> <at> if you a barefeet bitch , dont come near me bitch ctfuuu <first_speaker> <at> cthu yes we was just in the shop arguing ! <second_speaker> <at> lol omg why\n",
      "\n",
      "<first_speaker> <at> lmao yeah that one time where you was like dang , bby can i get your number lol and do your feet taste as good as they look ? lmao\n",
      "=====\n",
      "Human\n",
      "<first_speaker> is it just me or is the new <number> . <number> report a tad excruciating ? <second_speaker> <at> i miss kerry and that little twinkle in his eye .\n",
      "\n",
      "<first_speaker> <at> nothing will beat kerry , really . are leigh and chris just trying to be him ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> my life revolves around . android phones supra shoes work and tight vagina wordtomuvva <second_speaker> <at> ugh if i had a friggin laser beam you 'd be gone in a matter of seconds ! ! ! lmao\n",
      "\n",
      "<first_speaker> <at> if i had a duck i would pull it out and sick it on yu . yo !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> corey gunz might be the sickest . . <second_speaker> <at> ohwait\n",
      "\n",
      "<first_speaker> <at> he is , lol\n",
      "=====\n",
      "Human\n",
      "<first_speaker> they 're trying to get him and courtney lee <heart> <at> da bulls need to make a move for o . j . mayo . <heart>\n",
      "\n",
      "<second_speaker> <at> they need to make that happen .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> how ? rt <at> <at> <at> gay porn is so pointless <second_speaker> <at> <at> fuck you mean how ? <third_speaker> <at> <at> i love to see a bitch suck toes ! <first_speaker> <at> <at> ew why ? <third_speaker> <at> <at> not just any toes ! these bitches be having the crispness toessssss ! and yes they don 't that . i watch asian <number> <first_speaker> <at> <at> this one girl fucked another girl with her toe . i was confused as shit . like , how the hell is she cumming from this <second_speaker> <at> <at> link ! <third_speaker> <at> <at> lmfaooo jojo . you know how you fist a girl . i saw this girl get ' footed ' idk if there is an actual name <first_speaker> <at> <at> lmao . this one girl came from having her toes sucked doe .\n",
      "\n",
      "<second_speaker> <at> <at> wtf ? hoes can 't take it like they used too\n",
      "=====\n",
      "Human\n",
      "<first_speaker> can 't believe <at> has give it the large that he will smash me at fifa an i am tearing him apart excuses from him bad dontknowbuttons\n",
      "\n",
      "<second_speaker> <at> what your saying is true . look forward to playing you again soon ! i might not be the underdog . i think its <number> - <number> me in games\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> no what ? <second_speaker> <at> twitter does not need a like button .\n",
      "\n",
      "<second_speaker> <at> now what did you do ? ? ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> wake up . right now . <second_speaker> <at> arielle . <first_speaker> <at> yea ? <second_speaker> <at> are we gonna go to the park with nathan now ? <first_speaker> <at> mhm .\n",
      "\n",
      "<second_speaker> <at> i haven 't heard all of em .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> pay attention to the world around you . shit isn 't all cotton candy and lollipops .\n",
      "\n",
      "<second_speaker> i cant dance . <at> fuck you tired or are yu gay too ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> it 's bloody gorgeous weather winning\n",
      "\n",
      "<second_speaker> <at> perfect day for stimulating the tiger blood in your veins . heh\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> this needs to be watched . amazing song , amazing message - <url>\n",
      "\n",
      "<second_speaker> <at> that 's wassup what are you up to\n",
      "=====\n",
      "Human\n",
      "<first_speaker> cool , mom . i didnt think you 'd showup anyway .\n",
      "\n",
      "<second_speaker> <at> hey if i was in town i 'd say we all could have a big dinner or sonethin but i 'm in the roc -\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> thanks for the follow fellow writer ! !\n",
      "\n",
      "<second_speaker> <at> you too ! i 'm sure he will be upted in great time !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> don 't forget i have an ssd . i think a benchmark war is on and we should publicly display our results for twitter to decide .\n",
      "\n",
      "<first_speaker> <at> thanks jon for being super organized about the advbenchmark i don 't think it 'd go so well ! ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> im sooooo proud to be from barzeh barzawe and proud syria barzeh <second_speaker> <at> <first_speaker> <at> thank you very much for your kind wishes <second_speaker> <at> you are all heroes . i just pray god not to hear about bloodsheds and injured anymore .\n",
      "\n",
      "<first_speaker> <at> i know , i know . i just hope everything will be okay .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> had one ima get another one\n",
      "\n",
      "<second_speaker> <at> oh ok .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> do you want to see the concert on saturday or sunday ? <second_speaker> <at> a reservation seems to begin a concert ticket in japan . does thailand have not yet come ?\n",
      "\n",
      "<first_speaker> <at> yeah , of course . we were over there , but kinda rochester , next fri then dreamed of the tour again . i want to ask my mom , however .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> cont especially when luebke can work long relief and or come in to face lefties . make no mistake , luebke will take over in the rotation <second_speaker> <at> between moseley and luebke , i got to think leblanc is about <number> th on the sp depth chart at this point , no ?\n",
      "\n",
      "<first_speaker> <at> he 'd definitely behind moseley and luebke . deduno will go down to make a spot , then hunter for patterson .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> en route to work . i cant keep pressing snooze like this ! sheesh i pressed it <number> times this morning\n",
      "\n",
      "<second_speaker> <at> the snooze button has gotta be up there with sliced bread .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> blues ! the beam saber is complete ! <at> <second_speaker> <at> good ! good ! what does it do ? * holds saber * <first_speaker> <at> its length can be adjusted , it can turn into a beam whip , beam ball & chain , beam tonfa , and beam axe . ^ _ ^ <second_speaker> <at> great ! * looks at it * hey does it connect to my shield ? great ! <first_speaker> <at> yes ! and you will look like a gundam brother ^ _ ^\n",
      "\n",
      "<second_speaker> <at> lol ! ! ! ! ! ! ! ! ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> all my people love me . \" col muammar gaddafi\n",
      "\n",
      "<second_speaker> <at> except for the rats and the drug addicts , but they don 't really count .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> too many fuckin <number> s smh go to the basket ! ! ! ! go to the basket ! ! ! wtf yall shooting for ?\n",
      "\n",
      "<second_speaker> <at> fuck you ! ! ! !\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print('Human' if frame.iloc[i]['human-generated'] == 1 else 'Generated')\n",
    "    print(frame.iloc[i].context)\n",
    "    print('')\n",
    "    print(frame.iloc[i].response)\n",
    "    print('=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_list = []\n",
    "all_labels_list = []\n",
    "short_frame = frame[['context', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f extract_features extract_features(short_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_counters(frame):\n",
    "    global human_counter, generated_counter\n",
    "    labels = frame['human-generated'].values\n",
    "    labels_sum = labels.sum()\n",
    "    human_counter += labels_sum\n",
    "    generated_counter += frame.shape[0] - labels_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 5s\tChunk 10 over: 9s\tChunk 15 over: 13s\tChunk 20 over: 16s\tChunk 25 over: 21s\tChunk 30 over: 25s\tChunk 35 over: 29s\tChunk 40 over: 33s\tChunk 45 over: 37s\tChunk 50 over: 41s\tChunk 55 over: 45s\tChunk 60 over: 49s\tChunk 65 over: 53s\tChunk 70 over: 57s\tChunk 75 over: 61s\tChunk 80 over: 65s\tChunk 85 over: 68s\tChunk 90 over: 72s\tChunk 95 over: 76s\tChunk 100 over: 81s\tChunk 105 over: 85s\tChunk 110 over: 89s\tChunk 115 over: 92s\tChunk 120 over: 97s\tChunk 125 over: 101s\tChunk 130 over: 111s\tChunk 135 over: 124s\tChunk 140 over: 138s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "human_counter = 0\n",
    "generated_counter = 0\n",
    "run(write_counters, full_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3595488 3595488\n"
     ]
    }
   ],
   "source": [
    "print(human_counter, generated_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tags(frame):\n",
    "    responses_frame = frame['response']\n",
    "    \n",
    "    token_lists = responses_frame.str.split(' ').values\n",
    "    labels = frame['human-generated'].values\n",
    "    \n",
    "    for i, token_list in enumerate(token_lists):\n",
    "        tag_list = list(filter(lambda token: token[0] == '<' and token[-1] == '>', token_list))\n",
    "        tag_set = set(tag_list)\n",
    "        for tag in tag_set:\n",
    "            tag_count = tag_list.count(tag)\n",
    "            if labels[i] == 0:\n",
    "                tag_dict_generated[tag]['occ'] += 1\n",
    "                tag_dict_generated[tag][tag_count] += 1\n",
    "                tag_dict_generated[tag]['sum'] += tag_count\n",
    "            else:\n",
    "                tag_dict_human[tag]['occ'] += 1\n",
    "                tag_dict_human[tag][tag_count] += 1\n",
    "                tag_dict_human[tag]['sum'] += tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 18s\tChunk 10 over: 28s\tChunk 15 over: 34s\tChunk 20 over: 40s\tChunk 25 over: 46s\tChunk 30 over: 52s\tChunk 35 over: 59s\tChunk 40 over: 65s\tChunk 45 over: 71s\tChunk 50 over: 77s\tChunk 55 over: 84s\tChunk 60 over: 90s\tChunk 65 over: 96s\tChunk 70 over: 102s\tChunk 75 over: 108s\tChunk 80 over: 114s\tChunk 85 over: 120s\tChunk 90 over: 126s\tChunk 95 over: 133s\tChunk 100 over: 148s\tChunk 105 over: 164s\tChunk 110 over: 179s\tChunk 115 over: 192s\tChunk 120 over: 205s\tChunk 125 over: 219s\tChunk 130 over: 233s\tChunk 135 over: 246s\tChunk 140 over: 259s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "tag_dict_human = defaultdict(lambda: defaultdict(int))\n",
    "tag_dict_generated = defaultdict(lambda: defaultdict(int))\n",
    "run(write_tags, full_train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tag statistics saved in analysis_results/tag_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token_to_count in tokens_to_count:\n",
    "#     print('trying ' + token_to_count)\n",
    "#     model = train(train_filename)\n",
    "#     auc = estimate_auc(model)\n",
    "#     print('AUC ' + str(round(auc, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_punct(frame):\n",
    "    responses_frame = frame['response']\n",
    "    \n",
    "    token_lists = responses_frame.str.split(' ').values\n",
    "    labels = frame['human-generated'].values\n",
    "    \n",
    "    for i, token_list in enumerate(token_lists):\n",
    "        punct_list = list(filter(lambda token: len(token) == 1 and token in string.punctuation, token_list))\n",
    "        punct_set = set(punct_list)\n",
    "        for punct in punct_set:\n",
    "            punct_count = punct_list.count(punct)\n",
    "            if labels[i] == 0:\n",
    "                punct_dict_generated[punct]['occ'] += 1\n",
    "                punct_dict_generated[punct][punct_count] += 1\n",
    "                punct_dict_generated[punct]['sum'] += punct_count\n",
    "            else:\n",
    "                punct_dict_human[punct]['occ'] += 1\n",
    "                punct_dict_human[punct][punct_count] += 1\n",
    "                punct_dict_human[punct]['sum'] += punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 5s\tChunk 10 over: 12s\tChunk 15 over: 19s\tChunk 20 over: 25s\tChunk 25 over: 31s\tChunk 30 over: 38s\tChunk 35 over: 45s\tChunk 40 over: 52s\tChunk 45 over: 58s\tChunk 50 over: 65s\tChunk 55 over: 71s\tChunk 60 over: 77s\tChunk 65 over: 82s\tChunk 70 over: 89s\tChunk 75 over: 95s\tChunk 80 over: 101s\tChunk 85 over: 107s\tChunk 90 over: 113s\tChunk 95 over: 119s\tChunk 100 over: 125s\tChunk 105 over: 131s\tChunk 110 over: 140s\tChunk 115 over: 156s\tChunk 120 over: 167s\tChunk 125 over: 173s\tChunk 130 over: 179s\tChunk 135 over: 185s\tChunk 140 over: 191s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "punct_dict_human = defaultdict(lambda: defaultdict(int))\n",
    "punct_dict_generated = defaultdict(lambda: defaultdict(int))\n",
    "run(write_punct, full_train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation statistics saved in analysis_results/punc_analysis.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_features_labels(frame):\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    labels = frame['human-generated'].values.tolist()\n",
    "    \n",
    "    all_features_list.extend(features) # actually more efficient than numpy append\n",
    "    all_labels_list.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eval():\n",
    "    frame = first_frame(eval_filename)\n",
    "    \n",
    "    short_frame = frame[['context', 'response']]\n",
    "    features = np.array(extract_features(short_frame))\n",
    "    \n",
    "    truth = frame['human-generated'].values\n",
    "    \n",
    "    return (features, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(all_features, all_labels):\n",
    "    eval_features, eval_labels = get_eval()\n",
    "    print('Got data for early stopping')\n",
    "    \n",
    "    model = XGBClassifier(max_depth = 7, n_estimators = 200)\n",
    "    model = model.fit(all_features, all_labels, eval_set = [(eval_features, eval_labels)],\n",
    "                          eval_metric = 'auc', early_stopping_rounds = 5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def auc # custom auc with rounding for correct early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_auc(model):\n",
    "    features, truth = get_eval()\n",
    "    pred = model.predict(features)\n",
    "    return roc_auc_score(truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(filename):\n",
    "    global all_features_list, all_labels_list\n",
    "    \n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    run(write_features_labels, train_filename)\n",
    "    all_features = np.array(all_features_list)\n",
    "    all_labels = np.array(all_labels_list)\n",
    "    \n",
    "    model = get_model(all_features, all_labels)\n",
    "    \n",
    "    return all_features, all_labels, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for early stopping\n",
      "[0]\tvalidation_0-auc:0.707191\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.714476\n",
      "[2]\tvalidation_0-auc:0.715269\n",
      "[3]\tvalidation_0-auc:0.717607\n",
      "[4]\tvalidation_0-auc:0.719292\n",
      "[5]\tvalidation_0-auc:0.720185\n",
      "[6]\tvalidation_0-auc:0.721829\n",
      "[7]\tvalidation_0-auc:0.724163\n",
      "[8]\tvalidation_0-auc:0.725393\n",
      "[9]\tvalidation_0-auc:0.727011\n",
      "[10]\tvalidation_0-auc:0.728322\n",
      "[11]\tvalidation_0-auc:0.728999\n",
      "[12]\tvalidation_0-auc:0.730659\n",
      "[13]\tvalidation_0-auc:0.731461\n",
      "[14]\tvalidation_0-auc:0.73246\n",
      "[15]\tvalidation_0-auc:0.732988\n",
      "[16]\tvalidation_0-auc:0.733588\n",
      "[17]\tvalidation_0-auc:0.734323\n",
      "[18]\tvalidation_0-auc:0.734759\n",
      "[19]\tvalidation_0-auc:0.735415\n",
      "[20]\tvalidation_0-auc:0.735683\n",
      "[21]\tvalidation_0-auc:0.736361\n",
      "[22]\tvalidation_0-auc:0.736891\n",
      "[23]\tvalidation_0-auc:0.737347\n",
      "[24]\tvalidation_0-auc:0.737745\n",
      "[25]\tvalidation_0-auc:0.738052\n",
      "[26]\tvalidation_0-auc:0.738321\n",
      "[27]\tvalidation_0-auc:0.738598\n",
      "[28]\tvalidation_0-auc:0.738785\n",
      "[29]\tvalidation_0-auc:0.738917\n",
      "[30]\tvalidation_0-auc:0.739493\n",
      "[31]\tvalidation_0-auc:0.739742\n",
      "[32]\tvalidation_0-auc:0.740163\n",
      "[33]\tvalidation_0-auc:0.740446\n",
      "[34]\tvalidation_0-auc:0.74065\n",
      "[35]\tvalidation_0-auc:0.740923\n",
      "[36]\tvalidation_0-auc:0.74125\n",
      "[37]\tvalidation_0-auc:0.741452\n",
      "[38]\tvalidation_0-auc:0.741744\n",
      "[39]\tvalidation_0-auc:0.742079\n",
      "[40]\tvalidation_0-auc:0.742242\n",
      "[41]\tvalidation_0-auc:0.742633\n",
      "[42]\tvalidation_0-auc:0.742804\n",
      "[43]\tvalidation_0-auc:0.743148\n",
      "[44]\tvalidation_0-auc:0.743583\n",
      "[45]\tvalidation_0-auc:0.743772\n",
      "[46]\tvalidation_0-auc:0.744109\n",
      "[47]\tvalidation_0-auc:0.744392\n",
      "[48]\tvalidation_0-auc:0.744591\n",
      "[49]\tvalidation_0-auc:0.74478\n",
      "[50]\tvalidation_0-auc:0.745055\n",
      "[51]\tvalidation_0-auc:0.745365\n",
      "[52]\tvalidation_0-auc:0.745583\n",
      "[53]\tvalidation_0-auc:0.745826\n",
      "[54]\tvalidation_0-auc:0.746139\n",
      "[55]\tvalidation_0-auc:0.746272\n",
      "[56]\tvalidation_0-auc:0.746546\n",
      "[57]\tvalidation_0-auc:0.746718\n",
      "[58]\tvalidation_0-auc:0.746979\n",
      "[59]\tvalidation_0-auc:0.747225\n",
      "[60]\tvalidation_0-auc:0.747319\n",
      "[61]\tvalidation_0-auc:0.747452\n",
      "[62]\tvalidation_0-auc:0.747635\n",
      "[63]\tvalidation_0-auc:0.747906\n",
      "[64]\tvalidation_0-auc:0.748035\n",
      "[65]\tvalidation_0-auc:0.748141\n",
      "[66]\tvalidation_0-auc:0.748277\n",
      "[67]\tvalidation_0-auc:0.748589\n",
      "[68]\tvalidation_0-auc:0.748649\n",
      "[69]\tvalidation_0-auc:0.748751\n",
      "[70]\tvalidation_0-auc:0.748901\n",
      "[71]\tvalidation_0-auc:0.749009\n",
      "[72]\tvalidation_0-auc:0.749197\n",
      "[73]\tvalidation_0-auc:0.749289\n",
      "[74]\tvalidation_0-auc:0.74937\n",
      "[75]\tvalidation_0-auc:0.749498\n",
      "[76]\tvalidation_0-auc:0.749602\n",
      "[77]\tvalidation_0-auc:0.749683\n",
      "[78]\tvalidation_0-auc:0.749807\n",
      "[79]\tvalidation_0-auc:0.749921\n",
      "[80]\tvalidation_0-auc:0.750044\n",
      "[81]\tvalidation_0-auc:0.750115\n",
      "[82]\tvalidation_0-auc:0.750212\n",
      "[83]\tvalidation_0-auc:0.750337\n",
      "[84]\tvalidation_0-auc:0.750446\n",
      "[85]\tvalidation_0-auc:0.750482\n",
      "[86]\tvalidation_0-auc:0.750732\n",
      "[87]\tvalidation_0-auc:0.750896\n",
      "[88]\tvalidation_0-auc:0.750939\n",
      "[89]\tvalidation_0-auc:0.751095\n",
      "[90]\tvalidation_0-auc:0.75118\n",
      "[91]\tvalidation_0-auc:0.751294\n",
      "[92]\tvalidation_0-auc:0.751456\n",
      "[93]\tvalidation_0-auc:0.751524\n",
      "[94]\tvalidation_0-auc:0.751725\n",
      "[95]\tvalidation_0-auc:0.751763\n",
      "[96]\tvalidation_0-auc:0.751909\n",
      "[97]\tvalidation_0-auc:0.751999\n",
      "[98]\tvalidation_0-auc:0.752156\n",
      "[99]\tvalidation_0-auc:0.752234\n",
      "[100]\tvalidation_0-auc:0.752365\n",
      "[101]\tvalidation_0-auc:0.752485\n",
      "[102]\tvalidation_0-auc:0.752578\n",
      "[103]\tvalidation_0-auc:0.752669\n",
      "[104]\tvalidation_0-auc:0.752741\n",
      "[105]\tvalidation_0-auc:0.752815\n",
      "[106]\tvalidation_0-auc:0.752871\n",
      "[107]\tvalidation_0-auc:0.752886\n",
      "[108]\tvalidation_0-auc:0.752977\n",
      "[109]\tvalidation_0-auc:0.753146\n",
      "[110]\tvalidation_0-auc:0.753267\n",
      "[111]\tvalidation_0-auc:0.753411\n",
      "[112]\tvalidation_0-auc:0.753493\n",
      "[113]\tvalidation_0-auc:0.753554\n",
      "[114]\tvalidation_0-auc:0.753868\n",
      "[115]\tvalidation_0-auc:0.75392\n",
      "[116]\tvalidation_0-auc:0.754045\n",
      "[117]\tvalidation_0-auc:0.754147\n",
      "[118]\tvalidation_0-auc:0.75421\n",
      "[119]\tvalidation_0-auc:0.754286\n",
      "[120]\tvalidation_0-auc:0.754335\n",
      "[121]\tvalidation_0-auc:0.754368\n",
      "[122]\tvalidation_0-auc:0.754391\n",
      "[123]\tvalidation_0-auc:0.754414\n",
      "[124]\tvalidation_0-auc:0.754485\n",
      "[125]\tvalidation_0-auc:0.754579\n",
      "[126]\tvalidation_0-auc:0.754729\n",
      "[127]\tvalidation_0-auc:0.754839\n",
      "[128]\tvalidation_0-auc:0.754853\n",
      "[129]\tvalidation_0-auc:0.754953\n",
      "[130]\tvalidation_0-auc:0.75501\n",
      "[131]\tvalidation_0-auc:0.755048\n",
      "[132]\tvalidation_0-auc:0.75512\n",
      "[133]\tvalidation_0-auc:0.755136\n",
      "[134]\tvalidation_0-auc:0.755292\n",
      "[135]\tvalidation_0-auc:0.7554\n",
      "[136]\tvalidation_0-auc:0.755478\n",
      "[137]\tvalidation_0-auc:0.755531\n",
      "[138]\tvalidation_0-auc:0.755579\n",
      "[139]\tvalidation_0-auc:0.755615\n",
      "[140]\tvalidation_0-auc:0.755661\n",
      "[141]\tvalidation_0-auc:0.755689\n",
      "[142]\tvalidation_0-auc:0.755706\n",
      "[143]\tvalidation_0-auc:0.755773\n",
      "[144]\tvalidation_0-auc:0.755806\n",
      "[145]\tvalidation_0-auc:0.755867\n",
      "[146]\tvalidation_0-auc:0.755972\n",
      "[147]\tvalidation_0-auc:0.756018\n",
      "[148]\tvalidation_0-auc:0.75613\n",
      "[149]\tvalidation_0-auc:0.756167\n",
      "[150]\tvalidation_0-auc:0.756244\n",
      "[151]\tvalidation_0-auc:0.756264\n",
      "[152]\tvalidation_0-auc:0.756311\n",
      "[153]\tvalidation_0-auc:0.756356\n",
      "[154]\tvalidation_0-auc:0.756435\n",
      "[155]\tvalidation_0-auc:0.756573\n",
      "[156]\tvalidation_0-auc:0.756585\n",
      "[157]\tvalidation_0-auc:0.756622\n",
      "[158]\tvalidation_0-auc:0.756643\n",
      "[159]\tvalidation_0-auc:0.756636\n",
      "[160]\tvalidation_0-auc:0.756649\n",
      "[161]\tvalidation_0-auc:0.756651\n",
      "[162]\tvalidation_0-auc:0.75674\n",
      "[163]\tvalidation_0-auc:0.75679\n",
      "[164]\tvalidation_0-auc:0.756797\n",
      "[165]\tvalidation_0-auc:0.756809\n",
      "[166]\tvalidation_0-auc:0.756979\n",
      "[167]\tvalidation_0-auc:0.757074\n",
      "[168]\tvalidation_0-auc:0.757123\n",
      "[169]\tvalidation_0-auc:0.757126\n",
      "[170]\tvalidation_0-auc:0.757202\n",
      "[171]\tvalidation_0-auc:0.757261\n",
      "[172]\tvalidation_0-auc:0.757288\n",
      "[173]\tvalidation_0-auc:0.757332\n",
      "[174]\tvalidation_0-auc:0.757355\n",
      "[175]\tvalidation_0-auc:0.75736\n",
      "[176]\tvalidation_0-auc:0.757456\n",
      "[177]\tvalidation_0-auc:0.75746\n",
      "[178]\tvalidation_0-auc:0.757515\n",
      "[179]\tvalidation_0-auc:0.757506\n",
      "[180]\tvalidation_0-auc:0.757513\n",
      "[181]\tvalidation_0-auc:0.757541\n",
      "[182]\tvalidation_0-auc:0.75757\n",
      "[183]\tvalidation_0-auc:0.757597\n",
      "[184]\tvalidation_0-auc:0.757628\n",
      "[185]\tvalidation_0-auc:0.757711\n",
      "[186]\tvalidation_0-auc:0.757748\n",
      "[187]\tvalidation_0-auc:0.757802\n",
      "[188]\tvalidation_0-auc:0.757922\n",
      "[189]\tvalidation_0-auc:0.757998\n",
      "[190]\tvalidation_0-auc:0.758033\n",
      "[191]\tvalidation_0-auc:0.758045\n",
      "[192]\tvalidation_0-auc:0.758186\n",
      "[193]\tvalidation_0-auc:0.758214\n",
      "[194]\tvalidation_0-auc:0.758256\n",
      "[195]\tvalidation_0-auc:0.758272\n",
      "[196]\tvalidation_0-auc:0.758292\n",
      "[197]\tvalidation_0-auc:0.758278\n",
      "[198]\tvalidation_0-auc:0.758294\n",
      "[199]\tvalidation_0-auc:0.758294\n"
     ]
    }
   ],
   "source": [
    "model = get_model(all_features, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 26s\tChunk 5 over: 157s\tChunk 10 over: 286s\t\n",
      "14 chunks overall\n",
      "Got data for early stopping. Now fitting...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'eval_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-cbe9bc9c038a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-218-e747c369dc8c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-2c23e482f3ca>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(all_features, all_labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Got data for early stopping. Now fitting...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     model = XGBClassifier(max_depth = 7, n_estimators = 200, eval_set = (eval_features, eval_labels),\n\u001b[0;32m----> 6\u001b[0;31m                           eval_metric = 'auc', early_stopping_rounds = 30)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'eval_set'"
     ]
    }
   ],
   "source": [
    "all_features, all_labels, model = train(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open('6_pruned_early_stopping.pickle.dat', 'wb'))\n",
    "# model = pickle.load(open('44_features.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67939750103645391"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_auc(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.120897  ,  0.03973741,  0.05414512,  0.04833556,  0.03904026,\n",
       "        0.03816883,  0.02666589,  0.01748678,  0.01289723,  0.03410213,\n",
       "        0.02585255,  0.04153837,  0.0411317 ,  0.04984605,  0.05832801,\n",
       "        0.02614303,  0.06588044,  0.04409458,  0.04990414,  0.06338233,\n",
       "        0.10242259], dtype=float32)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min_child_weight, gamma, max_depth => model complexity\n",
    "# n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: initialize data inside for the functions to run and make them return values\n",
    "# TODO: reduce the number of features and store them in bytes, train on the whole dataset\n",
    "# TODO: compare distribution with context, implement bag of words and ngrams\n",
    "# TODO: grid search to tune XGBoost\n",
    "# TODO: http://www.nltk.org/book/ch05.html syntactical tagging including n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_ids(frame):\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    ids = frame['id'].values.tolist()\n",
    "    \n",
    "    all_features_list.extend(features) # actually more efficient than numpy append()\n",
    "    all_ids_list.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 32s\tChunk 5 over: 165s\tChunk 10 over: 277s\t\n",
      "11 chunks overall\n"
     ]
    }
   ],
   "source": [
    "all_features_list = []\n",
    "all_ids_list = []\n",
    "run(extract_features_ids, test_filename)\n",
    "all_features = np.array(all_features_list)\n",
    "all_ids = np.array(all_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.concatenate(([all_ids], [predicted_labels]), axis = 0).T, columns = ['id', 'human-generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

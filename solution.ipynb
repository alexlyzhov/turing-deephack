{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format\n",
    "===\n",
    "\n",
    "The data itself is already pre-processed (`<s>, </s>` tags, `<unk>` tag, etc.). The punctuation is tokenized (one symbol = one token, no word-punctuation merged tokens). The is only one space symbol between every two adjacent tokens.\n",
    "\n",
    "UPD: In the data there are common combination \"@@ \". This is because the data has been preprocessed with BPE encoding (see: https://arxiv.org/abs/1508.07909 and https://github.com/rsennrich/subword-nmt) in order to reduce the vocabulary size.\n",
    "\n",
    "In order to properly print a message you should make sure to do the following in Python:\n",
    "\n",
    "[your string message here].replace(‘@@ ‘, ‘’)\n",
    "\n",
    "NOTE: replace `@@space` by nothing. Don’t forget the [space]!\n",
    "\n",
    "Broadly speaking, BPE encoding will split words into the most common n-gram to reduce the vocabulary size. The ‘@@ ’ you see are tokens to indicate there was a split. Thus to print the actual word you should replace all occurrences of '@@' to nothing.\n",
    "\n",
    "\n",
    "Data fields are separated by one tab character.\n",
    "\n",
    "    context - context phrase(s) for response, always human generated\n",
    "    response - one phrase or a few phrases, may be from different speakers\n",
    "    human-generated - flag if the response is generated by human\n",
    "\n",
    "Data header: 'id\\tcontext\\tresponse\\thuman-generated\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission format\n",
    "===\n",
    "\n",
    "ROC AUC score.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "id,human-generated\n",
    "\n",
    "1,1\n",
    "\n",
    "8,0\n",
    "\n",
    "9,1\n",
    "\n",
    "10,1\n",
    "\n",
    "We expect the solution file to have 524,342 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # download corpora->words, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 50000\n",
    "train_filename = 'data/sampled_train.txt'\n",
    "full_train_filename = 'data/train.txt'\n",
    "test_filename = 'data/test.txt'\n",
    "eval_filename = 'data/eval.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(function, filename, print_every = 5, chunks = None):\n",
    "    start = time()\n",
    "    for i, frame in enumerate(pd.read_csv(filename, chunksize = chunksize, delimiter = '\\t')):\n",
    "        frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "        \n",
    "        function(frame)\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print('Chunk ' + str(i) + ' over: ' + str(round(time() - start)) + 's', end = '\\t')\n",
    "        if chunks is not None:\n",
    "            if i >= chunks - 1:\n",
    "                break\n",
    "    print('\\n' + str(i + 1) + ' chunks overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(frame):\n",
    "    frame_features = []\n",
    "    \n",
    "#     for index, row in frame.iterrows():\n",
    "#     for row in frame.itertuples(): # requires 'dot access' to columns\n",
    "#     zip is faster than itertuples and a whole lot faster than iterrows\n",
    "    for context, response in zip(frame['context'], frame['response']):\n",
    "        features = []\n",
    "        \n",
    "        # context length and number of tokens in context are usually useless\n",
    "        \n",
    "        response_tokens = response.split(' ')\n",
    "        context_tokens = context.split(' ')\n",
    "    \n",
    "        # feature: response length\n",
    "        features.append(float(len(response)))\n",
    "\n",
    "        # feature: number of tokens in the response\n",
    "        features.append(float(len(response_tokens)))\n",
    "        \n",
    "        # features: number of words of length 1, 2, ..., 5\n",
    "        # counters of length 6+ don't seem to have importance\n",
    "        lens = Counter(map(len, response_tokens))\n",
    "        for i in range(5):\n",
    "            features.append(float(lens[i + 1]))\n",
    "        \n",
    "        # features: number of specific tokens from the list in the response\n",
    "        tokens_to_count = ['<at>', '<number>', '!', '.']\n",
    "        for token_to_count in tokens_to_count:\n",
    "            features.append(float(response_tokens.count(token_to_count)))\n",
    "            \n",
    "        # features: includes apostrophes, is composed of english letters, is in vocabulary, is a stopword\n",
    "        # for context and response\n",
    "        apostrophe_counter = 0\n",
    "        alpha_counter = 0\n",
    "        vocab_counter = 0\n",
    "        stop_counter = 0\n",
    "        counters = [0] * 8\n",
    "        for token in response_tokens:\n",
    "            if \"'\" in token:\n",
    "                counters[0] += 1\n",
    "            if token.isalpha():\n",
    "                counters[1] += 1\n",
    "            if token in vocab:\n",
    "                counters[2] += 1\n",
    "            if token in stopwords:\n",
    "                counters[3] += 1\n",
    "        for token in context_tokens:\n",
    "            if \"'\" in token:\n",
    "                counters[4] += 1\n",
    "            if token.isalpha():\n",
    "                counters[5] += 1\n",
    "            if token in vocab:\n",
    "                counters[6] += 1\n",
    "            if token in stopwords:\n",
    "                counters[7] += 1\n",
    "        for counter in counters:\n",
    "            features.append(float(counter))\n",
    "            \n",
    "            \n",
    "        all_tokens = set(context_tokens + response_tokens)\n",
    "        shared_tokens = set(context_tokens).intersection(response_tokens)\n",
    "\n",
    "        # feature: number of shared tokens between context and response\n",
    "        features.append(float(len(shared_tokens)))\n",
    "\n",
    "        # feature: sum of abs diffs in token shares\n",
    "        diff = 0\n",
    "        for token in all_tokens:\n",
    "            token_stat = abs(float(context_tokens.count(token)) / len(context_tokens) -\n",
    "                         float(response_tokens.count(token)) / len(response_tokens))\n",
    "            diff += token_stat\n",
    "        features.append(diff)\n",
    "        \n",
    "        frame_features.append(features)\n",
    "\n",
    "    return frame_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_frame(print_filename):\n",
    "    for frame in pd.read_csv(print_filename, chunksize = chunksize, delimiter = '\\t'):\n",
    "        frame = frame.replace({'@@ ': ''}, regex = True)\n",
    "        break\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = first_frame(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated\n",
      "<first_speaker> sounds like many of my aie peeps had a crappy day ! hugs to all\n",
      "\n",
      "<second_speaker> <at> i 'll post sex so might fall asleep . have a good nights sleep too i can do it all day * sunbathing on the sofa instead screams *\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> posting a video soon about the new album . excited nervous to see what you kittens think . any guesses to what the album name is ?\n",
      "\n",
      "<second_speaker> <at> what is it ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> so there 's a mouse at work .\n",
      "\n",
      "<second_speaker> <at> wtf it betta b the <number> w <number> legs & a ponytail ! not <number> legs w a long tail ! coughing umm yea can 't make it in <number> day !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> in new york concrete jungle wet dream tomato . <second_speaker> <at> go to bed la boy .\n",
      "\n",
      "<first_speaker> <at> shit seems like that many drinks are in the gym ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> official event hashtag is soslam <at> <at>\n",
      "\n",
      "<second_speaker> <at> <at> i 'm all over it ! at dulles , making my way !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> exercise makes me soooo hungry . seems counterproductive to me <number>\n",
      "\n",
      "<second_speaker> <at> i especially get that with swimming - makes me absolutely ravenous ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> isn 't it lovely to wake up an ikey this morning <at> ? ?\n",
      "\n",
      "<second_speaker> <at> your thoughts on the game and win ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> andy sixx is the hottest emo guy \" & lt - - poser saying six is with ones x & <at> is not emo or gothic . he s just himself . d\n",
      "\n",
      "<second_speaker> <at> evening - . how 's you ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> even tho i pay $ 10 \\ month for black tie warranty on my evo best buy says i may have to wait <number> days ? ! ? <at> this is unacceptable nophone <second_speaker> <at> sorry to hear about your phone . email us your info to twitter <at> . com and someone from my team will get back <number> u .\n",
      "\n",
      "<first_speaker> <at> thank you for the response ! just sent the email\n",
      "=====\n",
      "Human\n",
      "<first_speaker> making the album part <number> today i was writing lyrics for a great summer song ! i wrote <number> verses , the chorus and . <url>\n",
      "\n",
      "<second_speaker> <at> try it out on your lv audience !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> listening to elvis costello 's little triggers and i forgot just how good it is . elviscostello can do no wrong in my book . <second_speaker> <at> amen sister <first_speaker> <at> seriously best song writer ever and his voice just makes the heart ache so good lol elviscostello <second_speaker> <at> doesn 't it make u feel about <number> % smarter than everyone else knowing that ? it 's so easy to cite , say , dylan , but ec . brilliant <first_speaker> <at> right behind him though is mr tom waits , such a storyteller of humor & sentimentality . tomwaits martha <second_speaker> <at> omg . have you been reading my diary ? you 're much older too ! ! ! ! ! mylonglongsister ! ! ! <first_speaker> <at> born in the <number> s product of the <number> s haha but i dress like i am still <number> and that 's the age i 'm stickin with lol ! ! <second_speaker> <at> gonna push my luck here ? ? an idiot abroad ? ? ? ? rowan atkinson ? ? ? tff ? ? ? nick lowe \" ? ? ? mike leigh ? ? <first_speaker> <at> i follow <at> . and let 's not forget the kiwis neil finn tim finn split enz and flight if the conchords haha ! !\n",
      "\n",
      "<second_speaker> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at> <at>\n",
      "=====\n",
      "Human\n",
      "<first_speaker> if y 'all reproduce , you will make the most horrible people on the planet . i can 't even wrap my head around it . <second_speaker> <at> you best not be talkin bout me !\n",
      "\n",
      "<first_speaker> <at> haha you know i 'm not .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> thanks babe dddd here 's mine - <url> ddd\n",
      "\n",
      "<second_speaker> <at> your welcome -\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> its too easy to beef . i takes a man to be mature about certain situations\n",
      "\n",
      "<second_speaker> <at> hahaha follow me back\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> just clicked on ur profile to see pic your son took a bit bigger then saw ur \" about me or whatever it 's called \" love it ! ! lol <second_speaker> <at> i just want to make sure no one 's under any illusions before they start talking to me <first_speaker> <at> i think it 's great - <second_speaker> <at> thanks . how 's your bank holiday weekend ? <first_speaker> <at> good thanks . weather great . we r not doing anything special just plodding along . you ? <second_speaker> <at> yeah friday saturday were good , the boy had a bit of a hissy fit day today which sucked , but tomorrow 's another day <first_speaker> <at> how old is he ? i played footie v mine <number> day . parents v the u6s . got a big bruise from him running into me , kicking my calf - <second_speaker> <at> he 's <number> . we were playing kinect sports , his attention span isn 't really up to going and having a full game of football yet <first_speaker> <at> i was glad he was our <number> rd then as it made us more relaxed about it . i just let him lie there - until he gave up <second_speaker> <at> i always get on the verge of losing my rag , then think he 's a good lad and it 's only natural . makes it better <first_speaker> <at> it is natural as it is for parents to lose it at times , i shout but they very rarely take notice . <second_speaker> <at> i 've tried to be more calm . i 'm a grumpy , short - tempered bastard , but it wouldn 't be good for him to see me like that . hi twitter ! <first_speaker> <at> oh , i have little patience and like things my way . get angry quickly , passes quickly - they do get <number> see it as it 's me .\n",
      "\n",
      "<first_speaker> <at> called ' vante ' and he 's haaaaaaaawt . lmao . idk , just saw that on my tl ' . i didn 't know austin was <number> ? ! dead .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> well . who knew is my favourite song . beside sober <second_speaker> <at> and every time i heard fuckin ' perfect i don 't know why but i really cry <at> writes really good songs d\n",
      "\n",
      "<first_speaker> <at> i love it ! ! ! ! ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> he can 't whoop my ass rt <at> seriously ? imma tell yo bro what u jus said rt <at> shut that ish up <cont> <url>\n",
      "\n",
      "<second_speaker> <at> lol i 'm just saying\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> no one tweets in the morning wtf teamfollowback <second_speaker> <at> hahahah shutup ! ! ! ! ! <first_speaker> <at> lol i know i 'm bored . that 's crazy cuz i was just thinking about tweeting you teamfollowback\n",
      "\n",
      "<second_speaker> <at> lol ! ! ! ! ! ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> rt <at> omds so what 's the flipping score ? kmt & lt don 't worry go and make me a sandwich\n",
      "\n",
      "<second_speaker> <at> rio did you really just tweet me this ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> permanent marker on a white board . thanks <at> ! <url>\n",
      "\n",
      "<second_speaker> <at> you 're welcome . i 'm a huge fan of you .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> rt <at> <at> muah ! ! ! just got back in hope your day is going well xoxoo <second_speaker> <at> so far so good . how 's things there ? <first_speaker> <at> not bad at all checking things quick online and headed to bed <second_speaker> <at> i just finished lunch . <first_speaker> <at> nice ! ! so what was in the menu <second_speaker> <at> chinese\n",
      "\n",
      "<first_speaker> <at> nice ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> patient said \" daaaamn you done get fat , boy ! \" def gonna run today . asshole <second_speaker> <at> lmfao how kind of him\n",
      "\n",
      "<first_speaker> <at> def a female patient . she was like \" you a pretty boy , go get that weight back off\n",
      "=====\n",
      "Human\n",
      "<first_speaker> new video up with <at> going over the surgeon head . check it out ! <url> <at> <at> <at> <second_speaker> <at> could you give me your honest opinion . are the assault ag more protective than the g22 ag 's ? thank you <first_speaker> <at> i prefer the assault ags and that is what i currently wear <second_speaker> <at> thanks . just sold my shakedowns which i didnt like whatso ever , buying the assaults today .\n",
      "\n",
      "<first_speaker> awesome way to start my day ! rt <at> thanks . just sold my shakedowns which i didnt like whatso ever , buying the assaults today\n",
      "=====\n",
      "Human\n",
      "<first_speaker> 40 in the <at> for the manu game ! ! ! nufc <second_speaker> <at> when is the big game ? ? - <first_speaker> <at> what big game ? <second_speaker> <at> manu v nufc ? ? <first_speaker> <at> <number> th you going ? <second_speaker> <at> i 'd love to but with it being midweek not sure how much work i 've on . also ling way to travel . <first_speaker> <at> where do you stay ? <second_speaker> <at> live just outside bournemouth in dorset . <first_speaker> <at> wow that is a long way to travel ! take it you don 't make many games ? <second_speaker> <at> unfortunately not . partly money and travelling always been the problem . have to stick with pub and pint instead . <first_speaker> <at> well least its on sky for ya , i don 't expect us to get anything from it tho jajajaja <second_speaker> <at> yeah , i was hoping it would . always like an excuse to escape to the pub .\n",
      "\n",
      "<first_speaker> <at> your next <number> games are on sky cl quarter final , facup semi then the big one against the mighty nufc jajajaja pubcallingjames\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> heey <at> white and pointy p x haha . fashion statment of the year ! x <second_speaker> <at> ohh dear . it 's a something statement of the year xx <first_speaker> <at> pahahaha i want some p x <second_speaker> <at> enjoy yourself - you can have my share s l xx <first_speaker> <at> my share of what ? xxx <second_speaker> <at> the white pointy stuff l x <first_speaker> <at> haha l x yeah p x <second_speaker> <at> lol l oooohhh . i have a new obsession zayn malik . he 's so bloody gorgeous & lt <number> xx <first_speaker> <at> i havent quite grown the obsestion but i agree he is gorgeous & lt <number> x <second_speaker> <at> he 's my background on my desktop . it changes every half an hour . i just drool pathetically over it ' l xx <first_speaker> <at> haha p x im gunna change mine to rlo now x <second_speaker> <at> lol . they were my original one l i have <number> pictures of him ? obsessed ? l xx <first_speaker> <at> im proud of u lou not so obsesed with rlo . haha just like robyn was pround when i was over jb x l x <second_speaker> <at> but now im obsessed with rlo and zayn ! help ! ! l xx <first_speaker> <at> i love how ur obssesed with rlo and zayn not rlo and <number> d . l xx i used to be obsesed with <number> d and rlo <second_speaker> <at> * <number> more times ! x <first_speaker> <at> haha u no u gotta obsestion when all u think and tallk about is them l true hotness they r the same x <second_speaker> <at> they 're not the same - they 're very different , but they 're just equally as hot ! ! l xx <first_speaker> <at> yeah i mean the same hotness l & lt <number> im just watching videos of <number> d p x <second_speaker> <at> i think i must have seen every one that included zayn one million times l x <first_speaker> <at> haha p gotta love ' em x <second_speaker> <at> i reaally do ! l xx <first_speaker> <at> p xxx <second_speaker> <at> zayn or niall ? x <first_speaker> <at> both x <second_speaker> <at> pahaha ! if you had to choose one ? x\n",
      "\n",
      "<first_speaker> <at> but i want to d xxx\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> i love this picture of demi from tonight . <url> <second_speaker> <at> nice picture ! , where did you find it ?\n",
      "\n",
      "<first_speaker> <at> i don 't know , i just found it on youtube . i love it .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> everyone when silent when i was down with problems . haissh\n",
      "\n",
      "<second_speaker> <at> whaaat ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> <at> yall got see how ikea dresses now that i told her to stay out of my closet cthu barefeet <second_speaker> <at> if you a barefeet bitch , dont come near me bitch ctfuuu <first_speaker> <at> cthu yes we was just in the shop arguing ! <second_speaker> <at> lol omg why\n",
      "\n",
      "<first_speaker> <at> lmao yeah that one time where you was like dang , bby can i get your number lol and do your feet taste as good as they look ? lmao\n",
      "=====\n",
      "Human\n",
      "<first_speaker> is it just me or is the new <number> . <number> report a tad excruciating ? <second_speaker> <at> i miss kerry and that little twinkle in his eye .\n",
      "\n",
      "<first_speaker> <at> nothing will beat kerry , really . are leigh and chris just trying to be him ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> my life revolves around . android phones supra shoes work and tight vagina wordtomuvva <second_speaker> <at> ugh if i had a friggin laser beam you 'd be gone in a matter of seconds ! ! ! lmao\n",
      "\n",
      "<first_speaker> <at> if i had a duck i would pull it out and sick it on yu . yo !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> corey gunz might be the sickest . . <second_speaker> <at> ohwait\n",
      "\n",
      "<first_speaker> <at> he is , lol\n",
      "=====\n",
      "Human\n",
      "<first_speaker> they 're trying to get him and courtney lee <heart> <at> da bulls need to make a move for o . j . mayo . <heart>\n",
      "\n",
      "<second_speaker> <at> they need to make that happen .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> how ? rt <at> <at> <at> gay porn is so pointless <second_speaker> <at> <at> fuck you mean how ? <third_speaker> <at> <at> i love to see a bitch suck toes ! <first_speaker> <at> <at> ew why ? <third_speaker> <at> <at> not just any toes ! these bitches be having the crispness toessssss ! and yes they don 't that . i watch asian <number> <first_speaker> <at> <at> this one girl fucked another girl with her toe . i was confused as shit . like , how the hell is she cumming from this <second_speaker> <at> <at> link ! <third_speaker> <at> <at> lmfaooo jojo . you know how you fist a girl . i saw this girl get ' footed ' idk if there is an actual name <first_speaker> <at> <at> lmao . this one girl came from having her toes sucked doe .\n",
      "\n",
      "<second_speaker> <at> <at> wtf ? hoes can 't take it like they used too\n",
      "=====\n",
      "Human\n",
      "<first_speaker> can 't believe <at> has give it the large that he will smash me at fifa an i am tearing him apart excuses from him bad dontknowbuttons\n",
      "\n",
      "<second_speaker> <at> what your saying is true . look forward to playing you again soon ! i might not be the underdog . i think its <number> - <number> me in games\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> no what ? <second_speaker> <at> twitter does not need a like button .\n",
      "\n",
      "<second_speaker> <at> now what did you do ? ? ?\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> wake up . right now . <second_speaker> <at> arielle . <first_speaker> <at> yea ? <second_speaker> <at> are we gonna go to the park with nathan now ? <first_speaker> <at> mhm .\n",
      "\n",
      "<second_speaker> <at> i haven 't heard all of em .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> pay attention to the world around you . shit isn 't all cotton candy and lollipops .\n",
      "\n",
      "<second_speaker> i cant dance . <at> fuck you tired or are yu gay too ?\n",
      "=====\n",
      "Human\n",
      "<first_speaker> it 's bloody gorgeous weather winning\n",
      "\n",
      "<second_speaker> <at> perfect day for stimulating the tiger blood in your veins . heh\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> this needs to be watched . amazing song , amazing message - <url>\n",
      "\n",
      "<second_speaker> <at> that 's wassup what are you up to\n",
      "=====\n",
      "Human\n",
      "<first_speaker> cool , mom . i didnt think you 'd showup anyway .\n",
      "\n",
      "<second_speaker> <at> hey if i was in town i 'd say we all could have a big dinner or sonethin but i 'm in the roc -\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> thanks for the follow fellow writer ! !\n",
      "\n",
      "<second_speaker> <at> you too ! i 'm sure he will be upted in great time !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> don 't forget i have an ssd . i think a benchmark war is on and we should publicly display our results for twitter to decide .\n",
      "\n",
      "<first_speaker> <at> thanks jon for being super organized about the advbenchmark i don 't think it 'd go so well ! ! !\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> im sooooo proud to be from barzeh barzawe and proud syria barzeh <second_speaker> <at> <first_speaker> <at> thank you very much for your kind wishes <second_speaker> <at> you are all heroes . i just pray god not to hear about bloodsheds and injured anymore .\n",
      "\n",
      "<first_speaker> <at> i know , i know . i just hope everything will be okay .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> had one ima get another one\n",
      "\n",
      "<second_speaker> <at> oh ok .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> <at> do you want to see the concert on saturday or sunday ? <second_speaker> <at> a reservation seems to begin a concert ticket in japan . does thailand have not yet come ?\n",
      "\n",
      "<first_speaker> <at> yeah , of course . we were over there , but kinda rochester , next fri then dreamed of the tour again . i want to ask my mom , however .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> cont especially when luebke can work long relief and or come in to face lefties . make no mistake , luebke will take over in the rotation <second_speaker> <at> between moseley and luebke , i got to think leblanc is about <number> th on the sp depth chart at this point , no ?\n",
      "\n",
      "<first_speaker> <at> he 'd definitely behind moseley and luebke . deduno will go down to make a spot , then hunter for patterson .\n",
      "=====\n",
      "Human\n",
      "<first_speaker> en route to work . i cant keep pressing snooze like this ! sheesh i pressed it <number> times this morning\n",
      "\n",
      "<second_speaker> <at> the snooze button has gotta be up there with sliced bread .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> blues ! the beam saber is complete ! <at> <second_speaker> <at> good ! good ! what does it do ? * holds saber * <first_speaker> <at> its length can be adjusted , it can turn into a beam whip , beam ball & chain , beam tonfa , and beam axe . ^ _ ^ <second_speaker> <at> great ! * looks at it * hey does it connect to my shield ? great ! <first_speaker> <at> yes ! and you will look like a gundam brother ^ _ ^\n",
      "\n",
      "<second_speaker> <at> lol ! ! ! ! ! ! ! ! ! ! !\n",
      "=====\n",
      "Human\n",
      "<first_speaker> all my people love me . \" col muammar gaddafi\n",
      "\n",
      "<second_speaker> <at> except for the rats and the drug addicts , but they don 't really count .\n",
      "=====\n",
      "Generated\n",
      "<first_speaker> too many fuckin <number> s smh go to the basket ! ! ! ! go to the basket ! ! ! wtf yall shooting for ?\n",
      "\n",
      "<second_speaker> <at> fuck you ! ! ! !\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print('Human' if frame.iloc[i]['human-generated'] == 1 else 'Generated')\n",
    "    print(frame.iloc[i].context)\n",
    "    print('')\n",
    "    print(frame.iloc[i].response)\n",
    "    print('=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_list = []\n",
    "all_labels_list = []\n",
    "short_frame = frame[['context', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f extract_features extract_features(short_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_counters(frame):\n",
    "    global human_counter, generated_counter\n",
    "    labels = frame['human-generated'].values\n",
    "    labels_sum = labels.sum()\n",
    "    human_counter += labels_sum\n",
    "    generated_counter += frame.shape[0] - labels_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 5s\tChunk 10 over: 9s\tChunk 15 over: 13s\tChunk 20 over: 16s\tChunk 25 over: 21s\tChunk 30 over: 25s\tChunk 35 over: 29s\tChunk 40 over: 33s\tChunk 45 over: 37s\tChunk 50 over: 41s\tChunk 55 over: 45s\tChunk 60 over: 49s\tChunk 65 over: 53s\tChunk 70 over: 57s\tChunk 75 over: 61s\tChunk 80 over: 65s\tChunk 85 over: 68s\tChunk 90 over: 72s\tChunk 95 over: 76s\tChunk 100 over: 81s\tChunk 105 over: 85s\tChunk 110 over: 89s\tChunk 115 over: 92s\tChunk 120 over: 97s\tChunk 125 over: 101s\tChunk 130 over: 111s\tChunk 135 over: 124s\tChunk 140 over: 138s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "human_counter = 0\n",
    "generated_counter = 0\n",
    "run(write_counters, full_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3595488 3595488\n"
     ]
    }
   ],
   "source": [
    "print(human_counter, generated_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tags(frame):\n",
    "    responses_frame = frame['response']\n",
    "    \n",
    "    token_lists = responses_frame.str.split(' ').values\n",
    "    labels = frame['human-generated'].values\n",
    "    \n",
    "    for i, token_list in enumerate(token_lists):\n",
    "        tag_list = list(filter(lambda token: token[0] == '<' and token[-1] == '>', token_list))\n",
    "        tag_set = set(tag_list)\n",
    "        for tag in tag_set:\n",
    "            tag_count = tag_list.count(tag)\n",
    "            if labels[i] == 0:\n",
    "                tag_dict_generated[tag]['occ'] += 1\n",
    "                tag_dict_generated[tag][tag_count] += 1\n",
    "                tag_dict_generated[tag]['sum'] += tag_count\n",
    "            else:\n",
    "                tag_dict_human[tag]['occ'] += 1\n",
    "                tag_dict_human[tag][tag_count] += 1\n",
    "                tag_dict_human[tag]['sum'] += tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 18s\tChunk 10 over: 28s\tChunk 15 over: 34s\tChunk 20 over: 40s\tChunk 25 over: 46s\tChunk 30 over: 52s\tChunk 35 over: 59s\tChunk 40 over: 65s\tChunk 45 over: 71s\tChunk 50 over: 77s\tChunk 55 over: 84s\tChunk 60 over: 90s\tChunk 65 over: 96s\tChunk 70 over: 102s\tChunk 75 over: 108s\tChunk 80 over: 114s\tChunk 85 over: 120s\tChunk 90 over: 126s\tChunk 95 over: 133s\tChunk 100 over: 148s\tChunk 105 over: 164s\tChunk 110 over: 179s\tChunk 115 over: 192s\tChunk 120 over: 205s\tChunk 125 over: 219s\tChunk 130 over: 233s\tChunk 135 over: 246s\tChunk 140 over: 259s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "tag_dict_human = defaultdict(lambda: defaultdict(int))\n",
    "tag_dict_generated = defaultdict(lambda: defaultdict(int))\n",
    "run(write_tags, full_train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tag statistics saved in analysis_results/tag_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for token_to_count in tokens_to_count:\n",
    "#     print('trying ' + token_to_count)\n",
    "#     model = train(train_filename)\n",
    "#     auc = estimate_auc(model)\n",
    "#     print('AUC ' + str(round(auc, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_punct(frame):\n",
    "    responses_frame = frame['response']\n",
    "    \n",
    "    token_lists = responses_frame.str.split(' ').values\n",
    "    labels = frame['human-generated'].values\n",
    "    \n",
    "    for i, token_list in enumerate(token_lists):\n",
    "        punct_list = list(filter(lambda token: len(token) == 1 and token in string.punctuation, token_list))\n",
    "        punct_set = set(punct_list)\n",
    "        for punct in punct_set:\n",
    "            punct_count = punct_list.count(punct)\n",
    "            if labels[i] == 0:\n",
    "                punct_dict_generated[punct]['occ'] += 1\n",
    "                punct_dict_generated[punct][punct_count] += 1\n",
    "                punct_dict_generated[punct]['sum'] += punct_count\n",
    "            else:\n",
    "                punct_dict_human[punct]['occ'] += 1\n",
    "                punct_dict_human[punct][punct_count] += 1\n",
    "                punct_dict_human[punct]['sum'] += punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 1s\tChunk 5 over: 5s\tChunk 10 over: 12s\tChunk 15 over: 19s\tChunk 20 over: 25s\tChunk 25 over: 31s\tChunk 30 over: 38s\tChunk 35 over: 45s\tChunk 40 over: 52s\tChunk 45 over: 58s\tChunk 50 over: 65s\tChunk 55 over: 71s\tChunk 60 over: 77s\tChunk 65 over: 82s\tChunk 70 over: 89s\tChunk 75 over: 95s\tChunk 80 over: 101s\tChunk 85 over: 107s\tChunk 90 over: 113s\tChunk 95 over: 119s\tChunk 100 over: 125s\tChunk 105 over: 131s\tChunk 110 over: 140s\tChunk 115 over: 156s\tChunk 120 over: 167s\tChunk 125 over: 173s\tChunk 130 over: 179s\tChunk 135 over: 185s\tChunk 140 over: 191s\t\n",
      "144 chunks overall\n"
     ]
    }
   ],
   "source": [
    "punct_dict_human = defaultdict(lambda: defaultdict(int))\n",
    "punct_dict_generated = defaultdict(lambda: defaultdict(int))\n",
    "run(write_punct, full_train_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation statistics saved in analysis_results/punc_analysis.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_features_labels(frame):\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    labels = frame['human-generated'].values.tolist()\n",
    "    \n",
    "    all_features_list.extend(features) # actually more efficient than numpy append\n",
    "    all_labels_list.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_all_features_labels(filename):\n",
    "    global all_features_list, all_labels_list\n",
    "    \n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    run(write_features_labels, filename)\n",
    "    all_features = np.array(all_features_list)\n",
    "    all_labels = np.array(all_labels_list)\n",
    "    \n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eval_data():\n",
    "    frame = first_frame(eval_filename)\n",
    "    \n",
    "    short_frame = frame[['context', 'response']]\n",
    "    features = np.array(extract_features(short_frame))\n",
    "    \n",
    "    truth = frame['human-generated'].values\n",
    "    \n",
    "    return (features, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rounded_auc(pred_param, labels_param): # custom auc with rounding for correct early stopping\n",
    "    pred = 1.0 / (1.0 + np.exp(-pred_param))\n",
    "    labels = labels_param.get_label()\n",
    "    score = roc_auc_score(labels, pred)\n",
    "    return ('auc', round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(all_features, all_labels, eval_features, eval_labels, early_stopping_rounds = 3):\n",
    "    model = XGBClassifier(max_depth = 7, n_estimators = 200)\n",
    "    model = model.fit(all_features, all_labels, eval_set = [(eval_features, eval_labels)],\n",
    "                      eval_metric = rounded_auc, early_stopping_rounds = early_stopping_rounds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast AUC evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_auc(model, eval_features, eval_labels):\n",
    "    pred = model.predict(eval_features)\n",
    "    return roc_auc_score(eval_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 27s\tChunk 5 over: 165s\tChunk 10 over: 304s\t\n",
      "14 chunks overall\n"
     ]
    }
   ],
   "source": [
    "all_features, all_labels = extract_all_features_labels(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_features, open('all_features.pickle.dat', 'wb'))\n",
    "# model = pickle.load(open('44_features.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_labels, open('all_labels.pickle.dat', 'wb'))\n",
    "# model = pickle.load(open('44_features.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_features, eval_labels = get_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.707\n",
      "Will train until validation_0-auc hasn't improved in 3 rounds.\n",
      "[1]\tvalidation_0-auc:0.714\n",
      "[2]\tvalidation_0-auc:0.715\n",
      "[3]\tvalidation_0-auc:0.718\n",
      "[4]\tvalidation_0-auc:0.719\n",
      "[5]\tvalidation_0-auc:0.72\n",
      "[6]\tvalidation_0-auc:0.722\n",
      "[7]\tvalidation_0-auc:0.724\n",
      "[8]\tvalidation_0-auc:0.725\n",
      "[9]\tvalidation_0-auc:0.727\n",
      "[10]\tvalidation_0-auc:0.728\n",
      "[11]\tvalidation_0-auc:0.729\n",
      "[12]\tvalidation_0-auc:0.731\n",
      "[13]\tvalidation_0-auc:0.731\n",
      "[14]\tvalidation_0-auc:0.732\n",
      "[15]\tvalidation_0-auc:0.733\n",
      "[16]\tvalidation_0-auc:0.734\n",
      "[17]\tvalidation_0-auc:0.734\n",
      "[18]\tvalidation_0-auc:0.735\n",
      "[19]\tvalidation_0-auc:0.735\n",
      "[20]\tvalidation_0-auc:0.736\n",
      "[21]\tvalidation_0-auc:0.736\n",
      "[22]\tvalidation_0-auc:0.737\n",
      "[23]\tvalidation_0-auc:0.737\n",
      "[24]\tvalidation_0-auc:0.738\n",
      "[25]\tvalidation_0-auc:0.738\n",
      "[26]\tvalidation_0-auc:0.738\n",
      "[27]\tvalidation_0-auc:0.739\n",
      "[28]\tvalidation_0-auc:0.739\n",
      "[29]\tvalidation_0-auc:0.739\n",
      "[30]\tvalidation_0-auc:0.739\n",
      "Stopping. Best iteration:\n",
      "[27]\tvalidation_0-auc:0.739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(all_features, all_labels, eval_features, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open('7_custom_early_stopping.pickle.dat', 'wb'))\n",
    "# model = pickle.load(open('44_features.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66327135445737684"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_auc(model, eval_features, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13741936,  0.03032258,  0.08322581,  0.03774194,  0.0383871 ,\n",
       "        0.0316129 ,  0.02129032,  0.01709677,  0.00580645,  0.04774193,\n",
       "        0.03225806,  0.06709678,  0.03967742,  0.04387097,  0.06806452,\n",
       "        0.00774194,  0.05258064,  0.04193548,  0.05645161,  0.06193548,\n",
       "        0.07774194], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: feature engineering\n",
    "implement bag of words and ngrams\n",
    "http://www.nltk.org/book/ch05.html syntactical tagging including n-gram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: tuning\n",
    "min_child_weight, gamma, max_depth => model complexity\n",
    "n_estimators\n",
    "using grid search like hyperopt\n",
    "https://www.dataiku.com/learn/guide/code/python/advanced-xgboost-tuning.html\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: other\n",
    "store features in bytes, train on a larger fraction of the dataset\n",
    "initialize data inside for the functions to run and make them return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_ids(frame):\n",
    "    short_frame = frame[['context', 'response']]\n",
    "    \n",
    "    features = extract_features(short_frame)\n",
    "    ids = frame['id'].values.tolist()\n",
    "    \n",
    "    test_features_list.extend(features) # actually more efficient than numpy append()\n",
    "    test_ids_list.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 over: 32s\tChunk 5 over: 165s\tChunk 10 over: 277s\t\n",
      "11 chunks overall\n"
     ]
    }
   ],
   "source": [
    "test_features_list = []\n",
    "test_ids_list = []\n",
    "run(extract_features_ids, test_filename)\n",
    "test_features = np.array(test_features_list)\n",
    "test_ids = np.array(test_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.concatenate(([test_ids], [predicted_labels]), axis = 0).T, columns = ['id', 'human-generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
